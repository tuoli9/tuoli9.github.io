[{"title":"A Little Robustness Goes a LongWay：Leveraging Robust Features for Targeted Transfer Attacks","url":"/posts/639f/","content":"<p>A Little Robustness Goes a LongWay: Leveraging Robust Features for Targeted Transfer Attacks（NIPS2021）</p>\n<p>代码：尚未开源</p>\n<h2 id=\"提出的问题\">提出的问题</h2>\n<p>之前的研究指出目标攻击很难迁移[<a href=\"#%5B1%5D\">1</a>]，之前对迁移对抗样本的研究主要集中在改进生成成功的图像扰动的优化方法，本文研究了用于构建对抗样本的源神经网络对样本迁移的影响。本文提出略微鲁棒的CNN—被训练成对小的对抗扰动具有鲁棒性的网络，可以被利用来大幅提高目标对抗样本对不同架构的可迁移性，针对一个稍微鲁棒的CNN构建的目标对抗样本可以迁移到 transformer。同时每个单独的略微鲁棒的神经网络都能有效地将特征转移到所有测试的非鲁棒网络，这表明略微鲁棒的网络依赖于与每个非鲁棒网络重叠的特征，尽管任何特定的非鲁棒网络的特征并没有与所有其他非鲁棒网络严重重叠。</p>\n<span id=\"more\"></span>\n<p>此外，本文利用对抗迁移技术来检验哪些特征是由神经网络学习的。根据先前的工作[<a href=\"#%5B2%5D\">2</a>]，本文发现，在从非鲁棒（标准）到高鲁棒的分类器的频谱上，那些只有轻微鲁棒的分类器生成出最可能迁移的 representation-targeted 对抗样本，这表明轻微鲁棒的网络的特征与每个被测试的目标网络有很大的重叠。这可以解释为什么略微鲁棒的网络会产生更多可迁移对抗攻击，并且对于下游的迁移学习任务有更好的权重初始化。</p>\n<p>主要贡献：</p>\n<ul>\n<li>相对于标准（非鲁棒）网络产生的对抗样本，针对轻微鲁棒的CNN产生的对抗样本更容易迁移，这种迁移不仅扩展到其他CNN，也扩展到 transformer 结构。</li>\n<li>随着源网络鲁棒性的增加，目标对抗样本对对抗防御网络的可迁移性也有大幅提高。</li>\n<li>研究了对抗损失函数在产生可迁移的对抗样本中的作用。</li>\n<li>非鲁棒的神经网络没有表现出实质性的特征（表征）可迁移性，而稍微鲁棒的神经网络则表现出可迁移性。这有助于解释为什么略微鲁棒的神经网络能使目标对抗样本的可迁移性更强。</li>\n</ul>\n<h2 id=\"实验\">实验</h2>\n<h3 id=\"transferability-of-adversarial-examples\">Transferability of Adversarial Examples</h3>\n<p>从 ImageNet validation 随机挑选1000张图片，每张图片类别不同，随机为每张图片挑选目标类，保证目标类和原始类别不同。用TMDI-FGSM 方法迭代300步生成对抗样本。实验用到的 CNN 模型有：Xception，VGG，ResNet，Inception，MobileNet，DenseNet，NASNetLarge，EfficientNet，transformer 模型有：ViT，LeViT，CCT，CLIP</p>\n<h4 id=\"transferability-to-convolutional-network-classifiers\">Transferability to Convolutional Network Classifiers</h4>\n<p>针对具有小的鲁棒性参数的源网络进行优化的对抗样本，与非鲁棒性（<span class=\"math inline\">\\(\\epsilon = 0\\)</span>）网络的成功率相比（图<a href=\"#image-20220330163245061\">1</a>）,在目标和非目标设定中都提高了迁移成功率。成功率的峰值对于每一个CNN目标网络大致相同（<span class=\"math inline\">\\(\\epsilon = 0.1\\)</span>）</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/tWEpf2j1cvKS3aO.png\" alt=\"image-20220330163245061\"><figcaption aria-hidden=\"true\">image-20220330163245061</figcaption>\n</figure>\n<p>图 1：针对 ImageNet 分类器的目标迁移攻击成功率，使用相对于 \"鲁棒的 ResNet50 源模型优化的对抗样本。成功率是指被目标网络分分类为目标类的对抗样本的比例。值越高攻击越成功。Baseline 是指未受干扰的图像被分类为目标类别的比率。(最好以彩色查看）。</p>\n<p>当 <span class=\"math inline\">\\(\\epsilon\\)</span> 超过最佳值，攻击成功率会快速下降，我们假设，当源网络的鲁棒性增加到一定程度后，网络开始完全忽略（非鲁棒性）目标网络所依赖的许多非鲁棒性特征，因此攻击不会修改这些特征，从而降低攻击的成功率。</p>\n<h4 id=\"transformer-based-classifiers\">Transformer-Based Classifiers</h4>\n<p>以前的研究中使用非鲁棒源网络的方法在使用卷积源模型构建目标的可迁移的对抗样本时几乎完全无效（表1）。这表明，基于 transformer 的模型和非鲁棒卷积模型所学习的特征在很大程度上是不同的。</p>\n<p>使用略微鲁棒的 ResNet50 分类器作为源网络，可以极大地提高目标对抗样本对 transformer 分类器的迁移性。源网络的最佳鲁棒性参数对于目标 transformer 网络和目标卷积网络是不同的 ，尽管我们发现源模型中任何数量的鲁棒性（低于一个临界值）都会大大改善可迁移性。因此，在现实世界的黑箱攻击设置中—目标网络的架构是完全未知的—攻击者可以在源网络鲁棒性参数之间找到一个平衡点，使CNN分类器和基于 transformer 的分类器的迁移性能达到最佳。</p>\n<p>​ 表 1：<span class=\"math inline\">\\(l_{\\infty}\\)</span>范数，扰动上限 16/255</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/RGTV7ivhgqec6xY.png\" alt=\"image-20220330172625745\"><figcaption aria-hidden=\"true\">image-20220330172625745</figcaption>\n</figure>\n<p>表1也比较了三种不同损失下的目标攻击攻击成功率。</p>\n<p>对经过对抗训练的模型的攻击。用于攻击模型的对抗扰动大小设置的要比对抗训练让模型鲁棒的扰动数值要大，没有鲁棒性的模型作为源模型时对抗样本迁移失败，源模型的鲁棒性越高，攻击成功率也越高。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/1pouaWc7iqL2IBK.png\" alt=\"image-20220330201115016\"><figcaption aria-hidden=\"true\">image-20220330201115016</figcaption>\n</figure>\n<p>​ 图 2：对对抗训练的鲁棒模型的攻击（<span class=\"math inline\">\\(\\epsilon = 3\\)</span>），源模型是<span class=\"math inline\">\\(\\epsilon\\)</span> - robust ResNet50</p>\n<h3 id=\"adversarial-transferability-of-features-特征的对抗迁移\">Adversarial Transferability of Features 特征的对抗迁移</h3>\n<p>这一部分展示略微鲁棒的网络比非鲁棒的网络和更鲁棒的网络表现出更高程度的表征可迁移性。这可以直接解释为什么用略微鲁棒的源网络构建的目标类对抗样本更容易转移，因为用略微鲁棒的网络生成的对抗样本将大量地转移特征，因此将较少地依赖少数可能不存在于每个模型中的高度脆弱的特征。</p>\n<p>实验设计出发点：<span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(y\\)</span> 是两个在源模型特征表达层（倒数第二层）非常相似的两个样本，如果源模型对目标模型的特征迁移程度很高，那么 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(y\\)</span> 在目标模型的特征表达应该也非常相似。</p>\n<p>从 ImageNet 测试集随机选择10张图像作为目标<span class=\"math inline\">\\(x\\)</span>，选择990张图像作为初始图像<span class=\"math inline\">\\(y_0\\)</span>，这1000张图像类别互不相同，对于每一个源模型，每一个目标类别，用 <span class=\"math inline\">\\(y0\\)</span> 构造对抗样本，攻击算法和上一部分同为 TMDI-FGSM。对于每一个类别，990个 representation-targeted 的对抗样本 <span class=\"math inline\">\\(y\\)</span> 应该和 <span class=\"math inline\">\\(x\\)</span> 在源模型有相似的特征表达，通过度量（<span class=\"math inline\">\\(x\\)</span>,<span class=\"math inline\">\\(y\\)</span>）在目标模型的表达，可以度量源模型的特征迁移能力。</p>\n<p>采用两种度量相似性的方法，一种直观的方法是用 t-SNE 画出每一个 representation-targeted 对抗样本 <span class=\"math inline\">\\(y\\)</span> 在目标模型的特征表达向量（图 3）。每一种颜色代表一个类别，每一张图中的<span class=\"github-emoji\"><span>⭐</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span>代表每一个目标 t-SNE 嵌入后的特征表达，如果与某一颜色相关的每个 <span class=\"math inline\">\\(y\\)</span> 的目的网络表达都接近其对应的<span class=\"github-emoji\"><span>⭐</span><img src=\"https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8\" aria-hidden=\"true\" onerror=\"this.parent.classList.add('github-emoji-fallback')\"></span>，那么表达迁移性就很高。当表达转移性较低时，针对同一目标图像生成的目达表达对抗样本的是不相似的，因此在图中不会出现分组的情况。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/kzhH1pX6t4UeqIn.png\" alt=\"image-20220330211010465\"><figcaption aria-hidden=\"true\">image-20220330211010465</figcaption>\n</figure>\n<p><img src=\"https://s2.loli.net/2022/03/30/sbV8jXdxgnKcvrF.png\"></p>\n<p>图 3：使用特定 \"鲁棒性 \"的白盒 ResNet50 模型生成的目标表达的对抗样本在目标模型表达的t-SNE图。底部的10张图片是来自ImageNet 的图片，我们将其作为表达目标，如文中所述。(最好以彩色和放大的方式观看)。</p>\n<p>另外一种是采用平均余弦相似度度量(<span class=\"math inline\">\\(x\\)</span>,<span class=\"math inline\">\\(y\\)</span>)在目标模型表达的相似度（表 2），平均余弦相似度越高，表示从源网络到对应的目标网络的表达迁移程度越高。</p>\n<p>​ 表 2：平均余弦相似度，每一个值都是9900个 (<span class=\"math inline\">\\(x\\)</span>,<span class=\"math inline\">\\(y\\)</span>) 目标表达相似度的平均</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/j9HzX3twDYvf7kN.png\" alt=\"image-20220330212309132\"><figcaption aria-hidden=\"true\">image-20220330212309132</figcaption>\n</figure>\n<p>非鲁棒的模型生成的目标对抗样本，即使对抗分类结果成功迁移，但对于大部分目标模型（<span class=\"math inline\">\\(\\epsilon = 0\\)</span>），特征表达没有充分迁移。这意味着 非鲁棒网络的特征可能不会彼此严重重叠。表2和图3观察到用非鲁棒网络生成的对抗样本对 DenseNet121 有一定程度的表达迁移性，这可能解释了图1中观察到的对 DenseNet121 的高度类转迁移性。</p>\n<p>但从稍微鲁棒的 ResNet50 分类器到 CLIP 的表示转移性是很高的，原因可能是 CLIP 不是在传统的分类问题上训练出来的，而是被训练来对图像和相应的文本描述进行类似的编码有关。高表达迁移性说明了略微鲁棒的网络的特征与每个被测试的（非鲁棒的）目标网络的特征有很大的重叠。</p>\n<p>当网络以大的 \"<span class=\"math inline\">\\(\\epsilon\\)</span> \"进行对抗性训练时，它们学到的特征与非鲁棒网络的特征重叠的程度随着 \"$<span class=\"math inline\">\\(\"的增加而减少。我们推测，某些非鲁棒的特征会被具有足够大的 \"\\)</span>$ \"的鲁棒神经网络所忽略，因此当鲁棒性增加到一定程度时，非鲁棒网络的许多特征会被忽略，表达的可迁移性会下降。</p>\n<h2 id=\"总结与改进\">总结与改进</h2>\n<p>本文主要研究了略微鲁棒的模型可以促进目标表达对抗样本的迁移，可能的原因是略微鲁棒的模型表达和目标模型表达有很大的重叠，当源模型过于鲁棒时，会忽略对于目标模型分类重要的非鲁棒特征，造成攻击迁移率下降。这两条规律对 CNN 和 transformer 都适用。</p>\n<p><strong>参考文献</strong></p>\n<p><span id=\"[1]\">[1] Y. Liu, X. Chen, C. Liu, and D. Song. Delving into transferable adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.</span></p>\n<p><span id=\"[2]\">[2] J. M. Springer, M. Mitchell, and G. T. Kenyon. Adversarial perturbations are not so weird: Entanglement of robust and non-robust features in neural network classifiers. arXiv preprint arXiv:2102.05110, 2021.</span></p>\n","categories":["对抗攻击","目标攻击"],"tags":["对抗攻击"]},{"title":"AAAI2022对抗攻击&防御论文汇总","url":"/posts/5c19/","content":"<h2 id=\"aaai2022对抗攻击防御文章汇总\">AAAI2022对抗攻击&amp;防御文章汇总</h2>\n<p><a href=\"https://aaai-2022.virtualchair.net/index.html\">AAAI2022 (virtualchair.net)</a></p>\n<h3 id=\"攻击\">攻击</h3>\n<p>Learning to Learn Transferable Attack</p>\n<p>Towards Transferable Adversarial Attacks on Vision Transformers</p>\n<span id=\"more\"></span>\n<p>Sparse-RS: A Versatile Framework for Query-Efficient Sparse Black-Box Adversarial Attacks</p>\n<p>Shape Prior Guided Attack: Sparser Perturbations on 3D Point Clouds</p>\n<p>Adversarial Attack for Asynchronous Event-Based Data</p>\n<p>CLPA: Clean-Label Poisoning Availability Attacks Using Generative Adversarial Nets</p>\n<p>TextHoaxer: Budgeted Hard-Label Adversarial Attacks on Text</p>\n<p>Hibernated Backdoor: A Mutual Information Empowered Backdoor Attack to Deep Neural Networks</p>\n<p>Hard to Forget: Poisoning Attacks on Certified Machine Unlearning</p>\n<p>Attacking Video Recognition Models with Bullet-Screen Comments</p>\n<p>Context-Aware Transfer Attacks for Object Detection</p>\n<p>A Fusion-Denoising Attack on InstaHide with Data Augmentation</p>\n<p>FCA: Learning a 3D Full-Coverage Vehicle Camouflage for Multi-View Physical Adversarial Attack</p>\n<p>Backdoor Attacks on the DNN Interpretation System</p>\n<p>Blindfolded Attackers Still Threatening: Strict Black-Box Adversarial Attacks on Graphs</p>\n<p>Synthetic Disinformation Attacks on Automated Fact Verification Systems</p>\n<p>Adversarial Bone Length Attack on Action Recognition</p>\n<p>Improved Gradient Based Adversarial Attacks for Quantized Networks</p>\n<p>Saving Stochastic Bandits from Poisoning Attacks via Limited Data Verification</p>\n<p>Has CEO Gender Bias Really Been Fixed? Adversarial Attacking and Improving Gender Fairness in Image Search</p>\n<p>Boosting the Transferability of Video Adversarial Examples via Temporal Translation</p>\n<p>Learning Universal Adversarial Perturbation by Adversarial Example</p>\n<p><a href=\"https://aaai-2022.virtualchair.net/poster_AAAI882.html\">Making Adversarial Examples More Transferable and Indistinguishable</a></p>\n<p><a href=\"https://aaai-2022.virtualchair.net/poster_aaai2044\">Vision Transformers are Robust Learners</a></p>\n<h3 id=\"防御\">防御</h3>\n<p>Certified Robustness of Nearest Neighbors Against Data Poisoning and Backdoor Attacks</p>\n<p>Preemptive Image Robustification for Protecting Users Against Man-in-the-Middle Adversarial Attacks</p>\n<p>Practical Fixed-Parameter Algorithms for Defending Active Directory Style Attack Graphs</p>\n<p>When Can the Defender Effectively Deceive Attackers in Security Games?</p>\n<p>Robust Heterogeneous Graph Neural Networks against Adversarial Attacks</p>\n<p>Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation</p>\n<p>Consistency Regularization for Adversarial Robustness</p>\n<p><a href=\"https://aaai-2022.virtualchair.net/poster_AAAI2923.html\">Adversarial Robustness in Multi-Task Learning: Promises and Illusions</a></p>\n<p><a href=\"https://aaai-2022.virtualchair.net/poster_AAAI6178.html\">LogicDef: An Interpretable Defense Framework Against Adversarial Examples via Inductive Scene Graph Reasoning</a></p>\n<p><a href=\"https://aaai-2022.virtualchair.net/poster_AAAI1772.html\">Efficient Robust Training via Backward Smoothing</a></p>\n<p><a href=\"https://aaai-2022.virtualchair.net/poster_AAAI1265.html\">Input-Specific Robustness Certification for Randomized Smoothing</a></p>\n<p><a href=\"https://aaai-2022.virtualchair.net/poster_AAAI10699.html\">CC-Cert: A Probabilistic Approach to Certify General Robustness of Neural Networks</a></p>\n","categories":["论文汇总"]},{"title":"ACM MM2022对抗攻击&防御论文汇总","url":"/posts/b824/","content":"<h2 id=\"acm-mm2022对抗攻击防御文章汇总\">ACM MM2022对抗攻击&amp;防御文章汇总</h2>\n<p><a href=\"https://scst.sysu.edu.cn/docs/20220718132716248974.pdf\">Imitated Detectors: Stealing Knowledge of Black-box Object Detectors</a></p>\n<p>Generating Transferable Adversarial Examples against Vision Transformers</p>\n","categories":["论文汇总"]},{"title":"Attacking deep networks with surrogate-based adversarial black-box methods is easy","url":"/posts/536b/","content":"<p>本文发表于ICLR2022，代码链接：https://github.com/fiveai/GFCS</p>\n<h3 id=\"摘要\">摘要</h3>\n<p>最近关于黑盒对抗性攻击的一系列工作，通过将其整合到基于查询的搜索中，恢复了对代替模型迁移的使用。然而，我们发现，现有的这种类型的方法没有发挥出它们的潜力，而且除此之外还可能过于复杂。在这里，我们提供了一个简单的算法，通过使用代替网络的类别分数梯度进行搜索，实现了最先进的结果，而不需要其他先验因素或启发式方法。该算法的指导性假设是，所研究的网络在基本意义上是在学习类似的功能，因此，从一个网络转移到另一个网络的攻击应该是相当 \"容易 \"的。这一假设被极低的查询次数和失败率所验证：例如，使用 ResNet-152 作为代理网络对 VGG-16 ImageNet 网络进行无目标的攻击，产生的中位查询次数为6，成功率为99.9%。</p>\n<span id=\"more\"></span>\n<h3 id=\"介绍\">介绍</h3>\n<p>本文针对 “score-based” 攻击，待攻击的受害者模型可以通过查询获得样本分类的类别和置信度。这类方法面临的最大问题是如何在不影响梯度估计质量的情况下，限制查询次数。\"score-based\"和\"transfer-based\"的攻击方法实际上是互补的：基于查询的策略可以从先验的、更有希望的搜索方向中获益，而基于转移的策略可以从灵活的优化器中获益，该优化器可以动态地纠正近似错误并考虑替代假设。此类的方法有[<a href=\"#%5B1%5D\">1</a>,<a href=\"#%5B2%5D\">2</a>,<a href=\"#%5B3%5D\">3</a>,<a href=\"#%5B4%5D\">4</a>]：</p>\n<p>本文提出的方法称为“GFCS：Gradient First, Coimage Second“，它的搜索方向代表了对抗损失本身的代用梯度，或者，就是从代用Jacobian 的行空间中随机选择的（称为 \"coimage\"）。前者代表标准的迁移攻击，后者涉及搜索局部线性替代模型表现出任何反应的特征空间，其本身就是一种广义的梯度转移形式。优化方法这里采用的仅仅是结合上下文的标准梯度投影上升（PGA）的SimBA[<a href=\"#%205\">5</a>]变体，方法效率的关键是识别有效的低维局部空间：对于普通ImageNet Inception-v3实现，将搜索限制在coimage 上， 将维度从299-299-3（输入分辨率）减少到1000（输出类的数量）。当然，损失梯度是一维的。</p>\n<p>从安全的角度来看，一个假设可以访问类别分数和替代模型的威胁模型是否是现实的，这是一个有争议的问题，我们对这个问题不了解。我们对这个问题的兴趣是分析性的，我们有两个主要观点要提出。首先，如果这个威胁模型被认为是有意义的（就像在现有技术中一样），那么就应该理解目前提出的问题是多么的 \"容易\"：即使是一个替代模型，也会把查询次数减少到可数。其次，GFCS在完全依靠网络间的梯度转移时表现出的事实表明，这些网络在一个重要意义上是非常相似的。</p>\n<h3 id=\"方法\">方法</h3>\n<p><img src=\"https://gitee.com/chawucir/pic/raw/master/img/image-20220328200330707.png\" alt=\"image-20220328200330707\" style=\"zoom:80%;\"></p>\n<p>上图就是GFCS的伪代码，第16行是PGA的实现，<span class=\"math inline\">\\(\\Pi_{\\mathbf{x}_{\\text {in }}, \\nu}\\)</span>是投影操作，算法中使用的损失是边际损失<span class=\"math inline\">\\(L_{\\mathbf{f}}(\\mathbf{x})=\\mathbf{f}_{c_{t}}(\\mathbf{x})-\\mathbf{f}_{c_{s}}(\\mathbf{x})\\)</span>，其中<span class=\"math inline\">\\(c_{s}=\\operatorname{argmax}_{c} \\mathbf{v}_{c}(\\mathbf{x})\\)</span> 并且<span class=\"math inline\">\\(c_{t}=\\operatorname{argmax} c_{c \\neq c_{s}} \\mathbf{V}_{c}(\\mathbf{x})\\)</span>，代表最高和第二高类别的置信度之差。注意到类别 <span class=\"math inline\">\\(c_t\\)</span> 和 <span class=\"math inline\">\\(c_s\\)</span> 在替代模型 <span class=\"math inline\">\\(\\text{v}\\)</span> 输出排序结果上定义，但损失通过网络 <span class=\"math inline\">\\(\\text{f}\\)</span> 的参数计算，<span class=\"math inline\">\\(\\text{f}\\)</span> 是 <span class=\"math inline\">\\(\\text{v}\\)</span> 或者 <span class=\"math inline\">\\(\\text{s}\\)</span> ，取决于算法第几步（算法第9,15行）。 对于替代模型的自然的假设是，替代模型能为受害模型提供有用的信息，但除了是映射<span class=\"math inline\">\\(\\mathcal{X}\\)</span>→<span class=\"math inline\">\\(\\mathcal{Y}\\)</span>的一次可微函数之外，对替代模型<span class=\"math inline\">\\(s\\in \\mathcal{S}\\)</span>没有 \"硬 \"要求。网络 f 和输入 x 对于ODS方向的定义<span class=\"math inline\">\\(\\text{d}_{\\text{ODS}}\\)</span> 和 [<a href=\"#%5B3%5D\">3</a>] 中一样。 <span class=\"math display\">\\[\n\\mathbf{d}_{\\mathrm{ODS}}(\\mathrm{x}, \\mathrm{f}, \\mathrm{w})=\\frac{\\nabla_{\\mathrm{x}}\\left(\\mathrm{w}^{\\top} \\mathbf{f}(\\mathrm{x})\\right)}{\\left\\|\\nabla_{\\mathbf{x}}\\left(\\mathrm{w}^{\\top} \\mathbf{f}(\\mathrm{x})\\right)\\right\\|_{2}}=\\frac{\\mathrm{w}^{\\top}\\left(\\nabla_{\\mathrm{x}} \\mathbf{f}(\\mathrm{x})\\right)}{\\left\\|\\nabla_{\\mathrm{x}}\\left(\\mathrm{w}^{\\top} \\mathbf{f}(\\mathrm{x})\\right)\\right\\|_{2}}\n\\]</span> 其中w是从<span class=\"math inline\">\\([-1, 1]^c\\)</span>上的均匀分布中取样的。根据定义，它是所有类别分数的随机加权和的归一化梯度。等价地，通过线性，它是所有类分数梯度（即Jacobian矩阵的行）的随机加权和，这些梯度本身是 <span class=\"math inline\">\\(\\text{f}\\)</span> 的线性近似的 coimage 的基准：<span class=\"math inline\">\\(\\text{f}\\)</span> 表现出任何非零响应的子空间。</p>\n<p>上述方法的逻辑很简单，在任何给定的迭代中，该方法试图以类似SimBA的方式进行，沿候选方向以固定步长的步骤测试对抗性损失的变化，必要时投影回可行集。它完全使用输入集合中的代用品的归一化损失梯度（以随机顺序抽取，没有替换），除非并且直到它在该迭代中用尽它们而没有成功。正如我们将在第3.2节中证明的那样，这种中间的失败状态很少会达到。然而，如果达到这种状态，该方法就会随机抽取一个替代（替换原来的扰动），并从该替代中抽取一个ODS方向，每次都尝试进行SimBA更新，直到实现损失的提高。一旦出现这种成功的更新，该方法就会将候选替代集重设为输入集，并恢复只使用归一化损失梯度。该方法在找到一个对抗样本或超过查询次数的上限时终止。</p>\n<h3 id=\"实验\">实验</h3>\n<h4 id=\"非目标攻击\">非目标攻击</h4>\n<p>本文将所提 GFCS 和[<a href=\"#%5B1%5D\">1</a>,<a href=\"#%5B3%5D\">3</a>,<a href=\"#%5B4%5D\">4</a>] 对比，所有的方法都是在 <span class=\"math inline\">\\(l_2\\)</span> 范数限制下的无目标攻击，数据集是2000张来自 ILSVR2012 验证集，能够被所有受害模型正确分类的样本。单个样本最大查询次数上限10,000，<span class=\"math inline\">\\(l_2\\)</span> 边界设置为 <span class=\"math inline\">\\(\\sqrt{0.001D}\\)</span> , <span class=\"math inline\">\\(D\\)</span> 是受害模型输入图像维度，受害模型采用 VGG-16，ResNet-50 和 Inception-v3。实验在两个替代模型集合重复：只有 ResNet-152，{VGG-19, ResNet-34, DenseNet-121, MobileNet-v2}，所有模型使用预训练的模型，可以从PyTorch/torchvision获取。LeBA 以 \"训练 \"模式在1000张图像中运行，然后以 \"测试 \"模式在所有其他方法使用的2000张图像中进行评估。P-RGF总是使用自适应系数模式。P-RGF和ODS-RGF是基于我们自己的PyTorch参考P-RGF代码，该代码将与本文一起发布：目前还没有ODS-RGF的公开实现。我们包括无替代的 [<a href=\"#6\">6</a>] 进行比较。</p>\n<p>表1报告了攻击成功率和查询次数的中位数，图2绘制了累积成功次数与每个样本花费的最大查询次数的对比图（CDFs，归一化模式）。与之前的工作不同，我们不报告平均值，因为这不适合总结这些方法产生的长尾分布。不确定性在表1中以标准误差表示，在图2中以95%的置信区间表示，这两种情况都是通过 bootstrap sampling 得到的。在表1中，有两点是很明显的。首先，所有研究的方法在这个问题上都有很高的成功率，针对所有的受害者网络：观察到的最低比率是Inception-v3 上 SimBA-ODS ，以ResNet-152作为唯一的替代模型。第二，GFCS 产生了极低的中位查询次数，同时取得了与所有其他方法类似的高成功率。这一事实可以在图2a、2b 和 2c 的单替代结果中更详细地看到，在这些结果中，GFCS 明显地在低查询系统中占优势，而在多个替代模型，图 2d、2e 和 2f 中则更加引人注目。尽管 GFCS 很简单：将 Alg. 1 和 LeBA 训练其替代模型的单独步骤进行比较。请注意，我们选择 SimBA-ODS 作为 coimage 采样器的部分原因是为了简化：正如结果所显示的，当它被单独使用时，会有非常少的失败发生，而我们则有效地继承了它们。但代价是在实现上增加了一些复杂性，例如 ODS-RGF 可以被替代，并可能导致失败率的进一步提高，同时仍然代表一种 GFCS 的形式。到还有一个现象注意：表1还显示了现有方法的性能对其自身参数选择的依赖性。惊人的是，ODS-RGF相对于早期的P-RGF的大部分经验优势是由于各自方法中默认参数的不同选择：当P-RGF仅仅使用ODS RGF 的默认参数时，它实际上在ResNet-50上的中位查询次数方面大大超过了它，代价是单一替代模型失败率增加了0.05%。</p>\n<p><img src=\"https://gitee.com/chawucir/pic/raw/master/img/image-20220328214208198.png\" alt=\"image-20220328214208198\" style=\"zoom:80%;\"></p>\n<p><img src=\"https://gitee.com/chawucir/pic/raw/master/img/image-20220328214234509.png\" alt=\"image-20220328214234509\" style=\"zoom:80%;\"></p>\n<p>SimBA-ODS从其步长参数的数量级增加中获益匪浅。这本身就进一步证明了我们的中心论点，即在这种情况下，转移通常被追求得太谨慎了。当然，我们也可以过于激进：请看表格中的消融线，它代表了对损失梯度的完全使用，即 \"没有CS的GF\"。</p>\n<h4 id=\"对算法表现的分析\">对算法表现的分析</h4>\n<p>为了深入研究第3.1节的结果，我们将每个被攻击的例子绘制成一个二维点，其x坐标是算法的代理损失梯度块所消耗的查询次数，其y坐标是 coimage 块的类似计数。这就得到了图3的散点图，对应于其对面轴的边际直方图来作为补充。该图显示了使用 Inception-v3 作为受害者的结果：VGG-16 和 ResNet-50 的类似数字见附录 A.3。请注意，主散点图的坐标轴用对数刻画，而边际直方图的轴是线性—对数。其一，有很大一部分样本（由图中底部密集的横排点代表）在很低的查询次数内（1-10次）就能成功，这完全或几乎完全是由于替代梯度转移，很少或根本没有使用ODS。由于这些低查询量的集群非常密集，为了量化这些集群，应该查阅相应的边际数据（最好在缩放状态下）。另外，当使用四种替代物集而不是单独使用ResNet-152时，这个体系之外的例子数量大大减少，通过比较图中的左右两边可以看出这一点。很明显，依靠基于梯度和基于 coimage 的之间的相互作用生成方向的样本被减少到一个非微不足道的（即如果不处理，足以影响失败率），但也是相对较小的一组。总的来说，在从底部密集的低查询集群延伸开来的点中，即依靠两种子方法的样本中，替代损失梯度查询和ODS查询有一个数量级的差异。也就是说，当需要ODS块时，它通常需要更多的查询来推进优化器，而在更常见的情况下，梯度就足够了。</p>\n<p><img src=\"https://gitee.com/chawucir/pic/raw/master/img/image-20220329140306612.png\" alt=\"image-20220329140306612\" style=\"zoom:80%;\"></p>\n<p>SimBA-ODS从其步长参数的数量级增加中获益匪浅。这本身就进一步证明了我们的中心论点，即在这种情况下，迁移在这种情况下过于谨慎。当然，我们也可以过于激进：见表中的消融线，代表完全使用损失梯度，也就是 \"没有 CS 的 GF \"。</p>\n<h4 id=\"目标攻击\">目标攻击</h4>\n<p>此部分实验设定和无目标攻击类似，目标类别为从 ImgeNet 1000个类别中均匀采样得到。对比实验方法这里仅对比了 SimBA-ODS [<a href=\"#%5B5%5D\">5</a>]和 Square Attack[<a href=\"#%5B6%5D\">6</a>]，其他方法都没有提供目标攻击的结果或代码。</p>\n<h3 id=\"参考文献\">参考文献</h3>\n<p><span id=\"[1]\">[1] Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. Advances in Neural Information Processing Systems, 32: 10934–10944, 2019.</span></p>\n<p><span id=\"[2]\">[2] Yiwen Guo, Ziang Yan, and Changshui Zhang. Subspace attack: Exploiting promising subspaces for query efficient black-box attacks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b.</span></p>\n<p><span id=\"[3]\">[3] Yusuke Tashiro, Yang Song, and Stefano Ermon. Diversity can be transferred: Output diversification for white-and black-box attacks. Advances in Neural Information Processing Systems, 33, 2020.</span></p>\n<p><span id=\"[4]\">[4] Jiancheng Yang, Yangzhou Jiang, Xiaoyang Huang, Bingbing Ni, and Chenglong Zhao. Learning black-box attackers with transferable priors and query feedback. Advances in Neural Information Processing Systems, 33, 2020.</span></p>\n<p><span id=\"[5]\">[5] Chuan Guo, Jacob Gardner, Yurong You, Andrew Gordon Wilson, and Kilian Weinberger. Simple black-box adversarial attacks. In International Conference on Machine Learning, pp. 2484–2493. PMLR, 2019a.</span></p>\n<p><span id=\"[6]\">[6] Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European Conference on Computer Vision, pp. 484–501. Springer, 2020.</span></p>\n","categories":["对抗攻击"],"tags":["对抗攻击"]},{"title":"CVPR2022对抗攻击&防御论文汇总","url":"/posts/e478/","content":"<h2 id=\"cvpr2022对抗攻击防御论文汇总\">CVPR2022对抗攻击&amp;防御论文汇总</h2>\n<p>先占个坑，本文持续更新，论文阅读笔记后续有时间就补。</p>\n<p>下面论文部分来自<a href=\"https://github.com/extreme-assistant/CVPR2022-Paper-Code-Interpretation/blob/master/CVPR2022.md#GAN\">这个链接</a></p>\n<span id=\"more\"></span>\n<h3 id=\"攻击\">攻击</h3>\n<p><a href=\"https://arxiv.org/abs/2203.03373\">Adversarial Texture for Fooling Person Detectors in the Physical World</a></p>\n<p>Adversarial Eigen Attack on Black-Box Models</p>\n<p>Bounded Adversarial Attack on Deep Content Features</p>\n<p>Backdoor Attacks on Self-Supervised Learning</p>\n<p>Bandits for Structure Perturbation-Based Black-Box Attacks To Graph Neural Networks With Theoretical Guarantees</p>\n<p>Boosting Black-Box Attack With Partially Transferred Conditional Adversarial Distribution</p>\n<p>BppAttack: Stealthy and Efficient Trojan Attacks Against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning</p>\n<p>Cross-Modal Transferable Adversarial Attacks From Images to Videos</p>\n<p>Can You Spot the Chameleon? Adversarially Camouflaging Images From Co-Salient Object Detection</p>\n<p><a href=\"https://arxiv.org/abs/2203.09831\">DTA: Physical Camouflage Attacks using Differentiable Transformation Network</a></p>\n<p><a href=\"https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_DST_Dynamic_Substitute_Training_for_Data-Free_Black-Box_Attack_CVPR_2022_paper.pdf\">DST: Dynamic Substitute Training for Data-Free Black-Box Attack</a></p>\n<p>Dual Adversarial Adaptation for Cross-Device Real-World Image Super-Resolution</p>\n<p>DetectorDetective: Investigating the Effects of Adversarial Examples on Object Detectors</p>\n<p>Exploring Effective Data for Surrogate Training Towards Black-Box Attack</p>\n<p><a href=\"https://arxiv.org/abs/2203.05151\">Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity</a></p>\n<p>Fairness-Aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models</p>\n<p>FIBA: Frequency-Injection Based Backdoor Attack in Medical Image Analysis</p>\n<p>Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations</p>\n<p>Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness</p>\n<p><a href=\"https://arxiv.org/abs/2203.09123\">Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input</a></p>\n<p>Shape-Invariant 3D Adversarial Point Clouds</p>\n<p><a href=\"https://openaccess.thecvf.com/content/CVPR2022/papers/Berger_Stereoscopic_Universal_Perturbations_Across_Different_Architectures_and_Datasets_CVPR_2022_paper.pdf\">Stereoscopic Universal Perturbations Across Different Architectures and Datasets</a></p>\n<p><a href=\"https://arxiv.org/abs/2203.03818\">Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon</a></p>\n<p>Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability</p>\n<p><a href=\"https://arxiv.org/pdf/2203.03121.pdf\">Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer</a></p>\n<p><a href=\"https://arxiv.org/pdf/2203.01925.pdf\">Label-Only Model Inversion Attacks via Boundary Repulsion</a></p>\n<p><a href=\"https://arxiv.org/pdf/2204.00008v1.pdf\">Improving Adversarial Transferability via Neuron Attribution-Based Attacks</a></p>\n<p>Improving the Transferability of Targeted Adversarial Examples Through Object-Based Diverse Input</p>\n<p>Investigating Top-k White-Box and Transferable Black-Box Attack</p>\n<p><a href=\"https://arxiv.org/pdf/2204.02738.pdf\">Masking Adversarial Damage: Finding Adversarial Saliency for Robust and Sparse Network</a></p>\n<p><a href=\"https://arxiv.org/pdf/2203.15230.pdf\">Zero-Query Transfer Attacks on Context-Aware Object Detectors</a></p>\n<p>Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free</p>\n<p><a href=\"https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Towards_Efficient_Data_Free_Black-Box_Adversarial_Attack_CVPR_2022_paper.pdf\">Towards Efficient Data Free Blackbox Adversarial Attack</a></p>\n<p>Transferable Sparse Adversarial Attack</p>\n<p>Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks</p>\n<p><a href=\"https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_DEFEAT_Deep_Hidden_Feature_Backdoor_Attacks_by_Imperceptible_Perturbation_and_CVPR_2022_paper.pdf\">DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints</a></p>\n<p>Exploring Frequency Adversarial Attacks for Face Forgery Detection</p>\n<p>360-Attack: Distortion-Aware Perturbations From Perspective-Views</p>\n<h3 id=\"防御\">防御</h3>\n<p>Enhancing Adversarial Training With Second-Order Statistics of Weights</p>\n<p>Enhancing Adversarial Robustness for Deep Metric Learning</p>\n<p>Improving Robustness Against Stealthy Weight Bit-Flip Attacks by Output Code Matching</p>\n<p>Improving Adversarially Robust Few-Shot Image Classification With Generalizable Representations</p>\n<p><a href=\"https://arxiv.org/abs/2111.12229\">Subspace Adversarial Training</a></p>\n<p>Segment and Complete: Defending Object Detectors Against Adversarial Patch Attacks With Robust Patch Detection</p>\n<p>Self-Supervised Learning of Adversarial Example: Towards Good Generalizations for Deepfake Detection</p>\n<p><a href=\"https://arxiv.org/abs/2203.08519\">Towards Practical Certifiable Patch Defense with Vision Transformer</a></p>\n<p><a href=\"https://arxiv.org/abs/2203.05154\">Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack</a></p>\n<p><a href=\"https://arxiv.org/pdf/2203.06616.pdf\">LAS-AT: Adversarial Training with Learnable Attack Strategy</a></p>\n<p><a href=\"https://openaccess.thecvf.com/content/CVPR2022/html/Li_Robust_Structured_Declarative_Classifiers_for_3D_Point_Clouds_Defending_Adversarial_CVPR_2022_paper.html\">Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks With Implicit Gradients</a></p>\n<p>ResSFL: A Resistance Transfer Framework for Defending Model Inversion Attack in Split Federated Learning</p>\n<p>Towards Robust Rain Removal Against Adversarial Attacks: A Comprehensive Benchmark Analysis and Beyond</p>\n<p>Defensive Patches for Robust Recognition in the Physical World</p>\n<p>Understanding and Increasing Efficiency of Frank-Wolfe Adversarial Training</p>\n<p>On Adversarial Robustness of Trajectory Prediction for Autonomous Vehicles</p>\n<p>EyePAD++: A Distillation-Based Approach for Joint Eye Authentication and Presentation Attack Detection Using Periocular Images</p>\n<h3 id=\"其他\">其他</h3>\n<p>Appearance and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and Beyond</p>\n<p>Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart</p>\n<p>Robust Combination of Distributed Gradients Under Adversarial Perturbations</p>\n<p>WarpingGAN: Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation</p>\n<p>Leveraging Adversarial Examples To Quantify Membership Information Leakage</p>\n","categories":["论文汇总"]},{"title":"ECCV2022对抗攻击&防御论文汇总","url":"/posts/3c67/","content":"<h2 id=\"eccv2022对抗攻击防御文章汇总\">ECCV2022对抗攻击&amp;防御文章汇总</h2>\n<p><a href=\"https://eccv2022.ecva.net/program/accepted-papers/\">Accepted papers | ECCV2022 (ecva.net)</a></p>\n<p>参考：<a href=\"https://zhuanlan.zhihu.com/p/565344799?utm_campaign=shareopn&amp;utm_medium=social&amp;utm_oi=914084645107277824&amp;utm_psn=1554446641083604992&amp;utm_source=wechat_session\">ECCV2022 对抗样本方面论文 - 知乎 (zhihu.com)</a></p>\n<h2 id=\"攻击\">攻击</h2>\n<p><a href=\"https://arxiv.org/abs/2207.05382\">Frequency Domain Model Augmentation for Adversarial Attack</a></p>\n<p><a href=\"https://arxiv.org/abs/2207.06202\">Adversarially-Aware Robust Object Detector</a></p>\n<p><a href=\"https://arxiv.org/abs/2203.13214\">A Perturbation-Constrained Adversarial Attack for Evaluating the Robustness of Optical Flow</a></p>\n<p><a href=\"https://arxiv.org/abs/2207.04718\">Physical Attack on Monocular Depth Estimation with Optimal Adversarial Patches</a></p>\n<span id=\"more\"></span>\n<p><a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640522.pdf\">Shape Matters: Deformable Patch Attack</a></p>\n<p><a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640594.pdf\">LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity</a></p>\n<p><a href=\"https://arxiv.org/abs/2107.01809\">Boosting Transferability of Targeted Adversarial Examples via Hierarchical Generative Networks</a></p>\n<p><a href=\"https://arxiv.org/abs/2111.13844\">Adaptive Image Transformations for Transfer-based Adversarial Attack</a></p>\n<p>AdvDO: Realistic Adversarial Attacks for Trajectory Prediction</p>\n<p><a href=\"https://arxiv.org/abs/2112.06569\">Triangle Attack: A Query-efficient Decision-based Adversarial Attack</a></p>\n<p>Adversarial Label Poisoning Attack on Graph Neural Networks via Label Propagation</p>\n<p><a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650311.pdf\">Exploiting the local parabolic landscapes of adversarial losses to accelerate black-box adversarial attack</a></p>\n<p><a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640611.pdf\">A Large-scale Multiple-objective Method for Black-box Attack against Object Detection</a></p>\n<p>Watermark Vaccine: Adversarial Attacks to Prevent Watermark Removal</p>\n<p>GradAuto: Energy-oriented Attack on Dynamic Neural Networks</p>\n<p><a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890306.pdf\">SegPGD: An Effective and Efficient Adversarial Attack for Evaluating and Boosting Segmentation Robustness</a></p>\n<p><a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136740053.pdf\">TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations</a></p>\n<p><a href=\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136650188.pdf\">Black-Box Dissector: Towards Erasing-based Hard-Label Model Stealing Attack</a></p>\n<h2 id=\"防御\">防御</h2>\n<p><a href=\"https://arxiv.org/pdf/2201.12765v2.pdf\">Improving Robustness by Enhancing Weak Subnets</a></p>\n<p><a href=\"https://arxiv.org/abs/2207.04718\">Decoupled Adversarial Contrastive Learning for Self-supervised Adversarial Robustness</a></p>\n<p><a href=\"https://arxiv.org/abs/2207.08859\">Prior-Guided Adversarial Initialization for Fast Adversarial Training</a></p>\n<p>Enhanced Accuracy and Robustness via Multi-Teacher Adversarial Distillation</p>\n<p><a href=\"https://arxiv.org/abs/2112.13551\">Learning Robust and Lightweight Model through Separable Structured Transformations</a></p>\n<p><a href=\"https://arxiv.org/abs/2112.09219\">All You Need is RAW: Defending Against Adversarial Attacks with Camera Image Pipelines</a></p>\n<p><a href=\"https://arxiv.org/abs/2112.00378\"><span class=\"math inline\">\\(l_{\\infty}\\)</span>Robustness and Beyond: Unleashing Efficient Adversarial Training</a></p>\n<p>One Size Does NOT Fit All: Data-Adaptive Adversarial Training</p>\n<p><a href=\"https://arxiv.org/abs/2209.01199\">Revisiting Outer Optimization in Adversarial Training</a></p>\n<p>Scaling Adversarial Training to Large Perturbation Bounds</p>\n<p>ViP: Unified Certified Detection and Recovery for Patch Attack with Vision Transformers</p>\n<p>Effective Presentation Attack Detection Driven by Face Related Task</p>\n<p>Adversarially-Aware Robust Object Detector</p>\n<p>Towards Efficient Adversarial Training on Vision Transformers</p>\n<p><a href=\"https://arxiv.org/abs/2209.01199\">Revisiting Outer Optimization in Adversarial Training</a></p>\n<h2 id=\"其他\">其他</h2>\n<p>RIBAC: Towards Robust and Imperceptible Backdoor Attack against Compact DNN</p>\n<p>An Invisible Black-box Backdoor Attack through Frequency Domain</p>\n<p>Exploring the Devil in Graph Spectral Domain for 3D Point Cloud Attacks</p>\n<p>Hardly Perceptible Trojan Attack against Neural Networks with Bit Flips</p>\n<p>Semi-Leak: Membership Inference Attacks Against Semi-supervised Learning</p>\n<p>Zero-Shot Attribute Attacks on Fine-Grained Recognition Models</p>\n<p>An Impartial Take to the CNN vs Transformer Robustness Contest</p>\n","categories":["论文汇总"]},{"title":"ICLR2022对抗攻击&防御论文汇总","url":"/posts/2bb2/","content":"<h2 id=\"iclr2022对抗攻击防御论文汇总\">ICLR2022对抗攻击&amp;防御论文汇总</h2>\n<p><a href=\"https://openreview.net/group?id=ICLR.cc/2022/Conference\">ICLR 2022 Conference | OpenReview</a></p>\n<h3 id=\"攻击\">攻击</h3>\n<p><a href=\"https://openreview.net/pdf?id=D6nH3719vZy\">On Improving Adversarial Transferability of Vision Transformers</a></p>\n<p><a href=\"https://openreview.net/forum?id=bYGSzbCM_i\">Online Adversarial Attacks</a></p>\n<span id=\"more\"></span>\n<p><a href=\"https://openreview.net/forum?id=Zf4ZdI4OQPV\">Attacking deep networks with surrogate-based adversarial black-box methods is easy</a></p>\n<p><a href=\"https://openreview.net/forum?id=gVRhIEajG1k\">Rethinking Adversarial Transferability from a Data Distribution Perspective</a></p>\n<p><a href=\"https://openreview.net/forum?id=73MEhZ0anV\">Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models</a></p>\n<p><a href=\"https://openreview.net/forum?id=B5XahNLmna\">Data Poisoning Won’t Save You From Facial Recognition</a></p>\n<p><a href=\"https://openreview.net/forum?id=DesNW4-5ai9\">Transferable Adversarial Attack based on Integrated Gradients</a></p>\n<p><a href=\"https://openreview.net/forum?id=28ib9tf6zhr\">Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?</a></p>\n<p><a href=\"https://openreview.net/forum?id=QkRV50TZyP\">Beyond ImageNet Attack: Towards Crafting Adversarial Examples for Black-box Domains</a></p>\n<p><a href=\"https://openreview.net/forum?id=Bn09TnDngN\">How to Inject Backdoors with Better Consistency: Logit Anchoring on Clean Data</a></p>\n<p><a href=\"https://openreview.net/forum?id=af1eUDdUVz\">Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent</a></p>\n<p><a href=\"https://openreview.net/forum?id=UMfhoMtIaP5\">Provably Robust Adversarial Examples</a></p>\n<h3 id=\"防御\">防御</h3>\n<p><a href=\"https://openreview.net/forum?id=W9G_ImpHlQd\">How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective</a></p>\n<p><a href=\"https://openreview.net/forum?id=gpp7cf0xdfN\">Reverse Engineering of Imperceptible Adversarial Image Perturbations</a></p>\n<p><a href=\"https://openreview.net/pdf?id=yeP_zx9vqNm\">Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks</a></p>\n<p><a href=\"https://openreview.net/forum?id=_5js_8uTrx1\">Towards Evaluating the Robustness of Neural Networks Learned by Transduction</a></p>\n<p><a href=\"https://openreview.net/forum?id=MSgB8D4Hy51\">Post-Training Detection of Backdoor Attacks for Two-Class and Multi-Attack Scenarios</a></p>\n<p><a href=\"https://openreview.net/forum?id=TySnJ-0RdKI\">Backdoor Defense via Decoupling the Training Process</a></p>\n<p><a href=\"https://openreview.net/forum?id=MeeQkFYVbzW\">Adversarial Unlearning of Backdoors via Implicit Hypergradient</a></p>\n<p><a href=\"https://openreview.net/forum?id=af1eUDdUVz\">Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent</a></p>\n<p><a href=\"https://openreview.net/forum?id=BmJV7kyAmg\">Towards Understanding the Robustness Against Evasion Attack on Categorical Data</a></p>\n<p><a href=\"https://openreview.net/forum?id=gJLEXy3ySpu\">Almost Tight L0-norm Certified Robustness of Top-k Predictions against Adversarial Perturbations</a></p>\n<p><a href=\"https://openreview.net/forum?id=OM_lYiHXiCL\">AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis</a></p>\n<p><a href=\"https://openreview.net/forum?id=cZAi1yWpiXQ\">Adversarial Robustness Through the Lens of Causality</a></p>\n<p><a href=\"https://openreview.net/forum?id=hcoswsDHNAW\">Fast AdvProp</a></p>\n<p><a href=\"https://openreview.net/forum?id=oU3aTsmeRQV\">Self-ensemble Adversarial Training for Improved Robustness</a></p>\n<p><a href=\"https://openreview.net/forum?id=TXsjU8BaibT\">Trigger Hunting with a Topological Prior for Trojan Detection</a></p>\n<p><a href=\"https://openreview.net/forum?id=UMfhoMtIaP5\">Provably Robust Adversarial Examples</a></p>\n<p><a href=\"https://openreview.net/forum?id=Dzpe9C1mpiv\">A Unified Wasserstein Distributional Robustness Framework for Adversarial Training</a></p>\n<p><a href=\"https://openreview.net/forum?id=tUa4REjGjTf\">On the Certified Robustness for Ensemble Models and Beyond</a></p>\n<p><a href=\"https://openreview.net/forum?id=jJOjjiZHy3h\">Defending Against Image Corruptions Through Adversarial Augmentations</a></p>\n<p><a href=\"https://openreview.net/forum?id=vJZ7dPIjip3\">Generalization of Neural Combinatorial Solvers Through the Lens of Adversarial Robustness</a></p>\n<p><a href=\"https://openreview.net/forum?id=YeShU5mLfLt\">On the Convergence of Certified Robust Training with Interval Bound Propagation</a></p>\n<p><a href=\"https://openreview.net/forum?id=WVX0NNVBBkV\">Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?</a><a href=\"https://openreview.net/forum?id=tD7eCtaSkR\">Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100</a></p>\n<p><a href=\"https://openreview.net/pdf?id=7gE9V9GBZaI\">Exploring Memorization in Adversarial Training</a></p>\n<p>？？？</p>\n<p><a href=\"https://openreview.net/forum?id=9L1BsI4wP1H\">Adversarially Robust Conformal Prediction</a></p>\n","categories":["论文汇总"]},{"title":"Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input","url":"/posts/cc97/","content":"<p>Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input (CVPR2022)</p>\n<p>代码：https://github.com/dreamflake/ODI</p>\n<h2 id=\"解决的问题\">解决的问题</h2>\n<p>本文要解决目标攻击迁移效果差的文体。数据增广是缓解迁移攻击对抗样本对源模型过拟合的一种方法，之前的工作使用简单的数据变换比如 resize ，限制了输入的多样性。本文的研究动机在于人类视觉在三维物体上的图像的卓越感知能力，如果一个图像足够清晰，人类可以在多个观察条件下识别物体内容，如果一个对抗样本对于目标网络看起来像目标类别，这个网络也同样将这个渲染成3D的物体识别为目标类。</p>\n<span id=\"more\"></span>\n<p>贡献点：</p>\n<ul>\n<li>提出 object-based diverse input (ODI) 方法，首次将3D物体作为2D对抗样本优化过程中的画布</li>\n<li>攻击成功率和3D物体有关（例如枕头或者杯子），对源物体的集成可以提高迁移攻击成功率。</li>\n<li>对于 ImageNet 数据集，所提方法将平均攻击成功率由28.3%提高到47.0%</li>\n<li>ODI 的方法对人脸验证任务同样有效</li>\n</ul>\n<h2 id=\"方法\">方法</h2>\n<p>本文提出一种ODI攻击，首先引入一个3D物体，将样本投影到它的表面，诱导这个渲染的物体在多种渲染环境下，例如不同光照和视角被分为目标类</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/A741TU2bNDatK8h.png\" alt=\"image-20220329221016779\"><figcaption aria-hidden=\"true\">image-20220329221016779</figcaption>\n</figure>\n<p>本文 ODI 过程是可微的，包括三个步骤</p>\n<ol type=\"1\">\n<li><p><strong>准备一个3D对抗网络布。</strong>随机从目标物体池中挑选一个物体，所挑选的物体有一个三角形网布，一个纹理图，一个边框，这个边框代表画布区域，对抗样本将会画在纹理图上。将纹理图用随机纯色填充，对抗样本 resize 后插入纹理图边框区域。</p></li>\n<li><p><strong>渲染环境设置。</strong>包括光照和相机设置。对于相机，固定内参，调整外参，相机角度有三种：elevation, azimuth, 和 tilt。3D 网布大小初始化，投影的图像可以占据它85%的空间。相机角度和距离在一个预设的区间内随机采样。</p>\n<p>本文选取了 point lights 这个照明模型，但是 directional lights 模型也可以使用。亮度和散射条件在一个预设的区间内随机采样。同样光照位置在基准位置随机添加位移。</p></li>\n<li><p><strong>渲染和与背景混合。</strong>在采样环境中渲染对抗三维网格，并将其与随机生成的背景图像混合，以创建最终的输出图像。</p></li>\n</ol>\n<p>ODI 的方法可以和 TI，MI，SI 等方法结合。算法流程图如下</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/S8kgGFKRH62CYna.png\" alt=\"image-20220329223739949\"><figcaption aria-hidden=\"true\">image-20220329223739949</figcaption>\n</figure>\n<h2 id=\"实验结果\">实验结果</h2>\n<p>这里贴一个实验结果，更多实验见原文</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/Bw5J2cUyfDXFZSI.png\" alt=\"image-20220329223855100\"><figcaption aria-hidden=\"true\">image-20220329223855100</figcaption>\n</figure>\n<p>主要的实验设定和[<a href=\"#%5B1%5D\">1</a>]相同，在一定范围内角度，距离和背景图可以提高攻击成功率。</p>\n<h2 id=\"总结\">总结</h2>\n<p>ODI-MI-TI-VT 可以实现SOTA，但是处理单张图像耗时我觉得有点无法接受。。。同样相机参数设置也对实验结果有一定影响。</p>\n<p><span id=\"[1]\">[1] Zhengyu Zhao, Zhuoran Liu, and Martha Larson. On success and simplicity: A second look at transferable targeted attacks. In Advances in Neural Information Processing Systems, 2021.</span></p>\n","categories":["对抗攻击","目标攻击"],"tags":["对抗攻击"]},{"title":"NIPS2022对抗攻击&防御论文汇总","url":"/posts/ecaa/","content":"<p>来自：<a href=\"https://neurips.cc/Conferences/2022/Schedule?type=Poster\">NeurIPS 2022</a></p>\n<h2 id=\"攻击\">攻击</h2>\n<p>On the Robustness of Deep Clustering Models: Adversarial Attacks and Defenses</p>\n<p>Adversarial Attack on Attackers: Post-Process to Mitigate Black-Box Score-Based Query Attacks</p>\n<p>GAMA: Generative Adversarial Multi-Object Scene Attacks</p>\n<p>BadPrompt: Backdoor Attacks on Continuous Prompts</p>\n<p>VoiceBox: Privacy through Real-Time Adversarial Attacks with Audio-to-Audio Models</p>\n<p>Towards Reasonable Budget Allocation in Untargeted Graph Structure Attacks via Gradient Debias</p>\n<p>Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal</p>\n<p>Revisiting Injective Attacks on Recommender Systems</p>\n<span id=\"more\"></span>\n<p>Perceptual Attacks of No-Reference lmage Quality Models with Human-in-the-Loop</p>\n<p>Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class</p>\n<p>Learning to Attack Federated Learning: A Model-based Reinforcement Learning Attack Framework</p>\n<p>Boosting the Transferability of Adversarial Attacks with Reverse Adversarial Perturbation</p>\n<p>Adv-Attribute: lnconspicuous and Transferable Adversarial Attack on Face Recognition</p>\n<p>Black box Attacks via Surrogate Ensemble Search</p>\n<p>Natural Color Fool : Towards Boosting Black-box Unrestricted Attacks</p>\n<p>Towards Lightweight Black-Box Attack Against Deep Neural Networks</p>\n<p>Practical Adversarial Attacks on Spatiotemporal Traffic Forecasting Models</p>\n<p>One-shot Neural Backdoor Erasing via Adversarial Weight Masking Pre-trained Adversarial Perturbations</p>\n<p>lsometric 3D Adversarial Examples in the Physical World</p>\n<p>Adv-Attribute: Inconspicuous and Transferable Adversarial Attack on Face Recognition</p>\n<h2 id=\"防御\">防御</h2>\n<p>MORA: Improving Ensemble Robustness Evaluation with Model Reweighing Attack</p>\n<p>Adversarial Robustness is at Odds with Lazy Training</p>\n<p>Defending Against Adversarial Attacks via Neural Dynamic System</p>\n<p>A2: Efficient Automated Attacker for Boosting Adversarial Training</p>\n<p>Randomized Channel Shuffling: Minimal-Overhead Backdoor Attack Detection without Clean Datasets</p>\n<p>Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attack</p>\n<p>Adversarial Training with Complementary Labels: On the Benefit of Gradually Informative Attacks</p>\n<p>Trap and Replace: Defending Backdoor Attacks by Trapping Them into an Easy-to-Replace Subnetwork</p>\n<p>Efficient Adversarial Training without Attacking: Worst-Case-Aware Robust Reinforcement Learning</p>\n<p>Formulating Robustness Against Unforeseen Attacks</p>\n<p>Alleviating Adversarial Attacks on Variational Autoencoders with MCMC</p>\n<p>Adversarial training for high-stakes reliability</p>\n<p>Phase Transition from Clean Training to Adversarial Training</p>\n<p>Why Do Artificially Generated Data Help Adversarial Robustness</p>\n<p>Toward Robust Spiking Neural Network Against Adversarial Perturbation</p>\n<p>MultiGuard: Provably Robust Multi-label Classification against Adversarial Examples</p>\n<p>SNN-RAT:Robustness-enhanced Spiking Neural Network through Regularized Adversarial Training</p>\n<p>A CloserLook at the Adversarial Robustness of Deep Equilibrium Models</p>\n<p>Make Some Noise: Reliable and Efficient Single-Step Adversarial Training</p>\n<p>CalFAT: Calibrated Federated Adversarial Training with Label Skewness</p>\n<p>Enhance the Visual Representation via Discrete Adversarial Training</p>\n<p>Explicit Tradeoffs between Adversarial and Natural Distributional Robustness</p>\n<p>Label Noise in Adversarial Training: A Novel Perspective to Study Robust Overfittingview</p>\n<p>Adversarialy Robust Learning: A Generic Minimax Optimal Learner and Characterization</p>\n<p>Boosting Barely Robust Learners: A New Perspective on Adversarial Robustness</p>\n<p>Stability Analysis and Generalization Bounds of Adversarial Training</p>\n<p>Efficient and Effective Augmentation Strategy for Adversarial Training</p>\n<p>lmproving Adversarial Robustness of Vision Transformers</p>\n<p>Random Normalization Aggregation for Adversarial Defense</p>\n<p>DISCo: Adversarial Defense with Local lmplicit Functions</p>\n<p>Synergy-of-Experts : Collaborate to Improve Adversarial Robustness</p>\n<p>ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints</p>\n<p>Rethinking Lipschitz Neural Networks for Certified L-infinity Robustness</p>\n<h2 id=\"其他\">其他</h2>\n<p><strong>Indicators of Attack Failure: Debugging and Improving Optimization of Adversarial Examples</strong></p>\n<p>Can AdversarialTraining Be Manipulated By Non-Robust Features?</p>\n<p><strong>A Characterization of Semi-Supervised Adversarially Robust PAC Learnability</strong></p>\n<p>Are AlphaZero-like Agents Robust to Adversarial Perturbations?</p>\n<p>On the Adversarial Robustness of Mixture of Experts</p>\n<p>Increasing Confidence in Adversarial Robustness Evaluations</p>\n<p>What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?</p>\n","categories":["论文汇总"]},{"title":"On the Robustness of Vision Transformers to Adversarial Examples","url":"/posts/e759/","content":"<h2 id=\"on-the-robustness-of-vision-transformers-to-adversarial-examplesiccv2021\">On the Robustness of Vision Transformers to Adversarial Examples（ICCV2021）</h2>\n<p>代码链接：https://github.com/MetaMain/ViTRobust</p>\n<p>本文研究了ViT鲁棒性，迁移安全性研究分为3点：</p>\n<ul>\n<li>在白盒和黑盒攻击场景下测试 transformer 鲁棒性</li>\n<li>对抗样本很难从 CNN 迁移到 transformer，基于这一发现分析 CNN 和 transformer 简单的集成防御的安全性。</li>\n<li>提出一种 self-attention blended gradient attack(SAGA)，简单集成防御模型对于白盒攻击时不安全的，在黑盒对抗攻击下，作者指出集成防御模型可以在不牺牲准确率的情况下，达到较好的鲁棒性。</li>\n</ul>\n<h3 id=\"vision-transformer白盒攻击\">Vision Transformer白盒攻击</h3>\n<p>攻击方法：测试6种不同的白盒攻击方法：FGSM，MIN，PGD，APGD(Auto Projected Gradient Descent[<a href=\"#%5B1%5D\">1</a>])，C&amp;W，当梯度masking或者梯度混淆出现时白盒攻击失败，Backward Pass Differentiable Approximation (BPDA)[<a href=\"#%5B2%5D\">2</a>]，解决上述问题。</p>\n<p>分类模型：ViT-B-32，ViT-B-16，ViT-L-16，ViT-R50，B代表模型复杂度，B模型有12层，L模型有24层。Big Transfer Models：BiT-M-R50 和 BiT-M-R101x3，ResNet-56 和ResNet-164，对于CIFAR-10 和 CIFAR-100，攻击8个模型，ViT-B-32，ViT-B-16，ViT-L-16，ViT-R50，BiT-M-R50，BiT-M-R101x3，ResNet-56 和 ResNet-164，对于ImageNet，攻击7个模型：ViT-B-16，ViT-L-16（图片大小224），ViT-L-16 (图片大小512)，BiT-M-R50，BiT-MR152x4，ResNet-50 和 ResNet-152。</p>\n<p>实验结果：</p>\n<p>表一</p>\n<p><img src=\"https://s2.loli.net/2022/04/21/br6u7qShJBWAIMZ.png\" alt=\"image-20220421204201334\" style=\"zoom:80%;\"></p>\n<p>ViT对白盒攻击没有鲁棒性，表中数值代表对抗样本分类准确率，Acc代表这个模型干净样本准确率。</p>\n<h3 id=\"vit迁移性研究\">ViT迁移性研究</h3>\n<p>表二</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/21/VaTbv5BQM72hqzn.png\" alt=\"image-20220421205559121\"><figcaption aria-hidden=\"true\">image-20220421205559121</figcaption>\n</figure>\n<p>表中每个数据取几种攻击方法最高迁移率。</p>\n<h3 id=\"saga\">SAGA</h3>\n<p>原文：</p>\n<p>Instead of focusing completely on optimizing over one of the models, SAGA focuses on breaking multiple models at once.</p>\n<p>攻击目标：Vision Transformers集合<span class=\"math inline\">\\(V\\)</span>，CNN集合<span class=\"math inline\">\\(K\\)</span>，在扰动<span class=\"math inline\">\\(\\epsilon\\)</span>边界条件下使<span class=\"math inline\">\\(V\\)</span>和<span class=\"math inline\">\\(K\\)</span>所有成员都错分。 <span class=\"math display\">\\[\nx_{a d v}^{(i+1)}=x_{a d v}^{(i)}+\\epsilon_{s} * \\operatorname{sign}\\left(G_{b l e n d}\\left(x_{a d v}^{(i)}\\right)\\right)\n\\]</span></p>\n<p><span class=\"math display\">\\[\nG_{b l e n d}\\left(x_{a d v}^{(i)}\\right)=\\sum_{k \\in K} \\alpha_{k} \\frac{\\partial L_{k}}{\\partial x_{a d v}^{(i)}}+\\sum_{v \\in V} \\alpha_{v} \\phi_{v} \\odot \\frac{\\partial L_{v}}{\\partial x_{a d v}^{(i)}}\n\\]</span></p>\n<p><span class=\"math inline\">\\(\\phi_{v}\\)</span> 是<span class=\"math inline\">\\(v^{th}\\)</span> transformer 的自注意力图，用 attention rollout [<a href=\"#%5B3%5D\">3</a>]计算如下 <span class=\"math display\">\\[\n\\phi_{v}=\\left(\\prod_{l=1}^{n_{l}}\\left[\\sum_{i=1}^{n_{h}}\\left(0.5 W_{l, i}^{(a t t)}+0.5 I\\right)\\right]\\right)\n\\]</span> <span class=\"math inline\">\\(n_h\\)</span> 是每层attention heads个数，<span class=\"math inline\">\\(n_l\\)</span> 是注意力层数，<span class=\"math inline\">\\(W_{l,i}^{(att)}\\)</span>是每个attention heads注意力权重矩阵，<span class=\"math inline\">\\(I\\)</span>是恒等矩阵，<span class=\"math inline\">\\(x\\)</span>是输入图像，这个公式考虑到注意力从transformer 每一层流动到下一层，包括跳跃链接的影响。平均来自同一层内不同attention heads的注意力值，注意力值在不同层之间递归地相乘。</p>\n<p>实验结果：</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/22/1h7Xl5Se4IvuyCB.png\" alt=\"image-20220422223042259\"><figcaption aria-hidden=\"true\">image-20220422223042259</figcaption>\n</figure>\n<p>对Vision Transformers 和 Big Transfer Models简单集成的攻击，使用1000张正确分类的图片，</p>\n<p>basic attack 是没有权重参数和自注意力的梯度的组合，SAGA攻击的贡献在于证明了Vision Transformers 和 Big Transfer Models简单集成对于白盒攻击是不安全的。SAGA攻击优于其他两种攻击。</p>\n<h3 id=\"黑盒安全性和迁移性\">黑盒安全性和迁移性</h3>\n<p>测试了 RayS attack[<a href=\"#%5B4%5D\">4</a>] 和 Adaptive Black-Box Attack[<a href=\"#%5B5%5D\">5</a>] 的安全性，最大查询次数10000，对于 Adaptive Black-Box Attack，攻击者可以获得全部训练数据，synthetic model 为 ViTB-32。Vision Transformer 和 Big Transfer model 的简单集成大幅提高了安全性。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/23/FVqsLCM23wApUbS.png\" alt=\"image-20220423185104889\"><figcaption aria-hidden=\"true\">image-20220423185104889</figcaption>\n</figure>\n<p><span id=\"[1]\">[1] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In International Conference on Machine Learning, pages 2206–2216. PMLR, 2020.</span></p>\n<p><span id=\"[2]\">[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, pages 274– 283, 2018.</span></p>\n<p><span id=\"[3]\">[3] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4190–4197, 2020.</span></p>\n<p><span id=\"[4]\">[4] Jinghui Chen and Quanquan Gu. Rays: A ray searching method for hard-label adversarial attack. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pages 1739–1747, 2020.</span></p>\n<p><span id=\"[5]\">[5] Kaleel Mahmood, Phuong Ha Nguyen, Lam M. Nguyen, Thanh Nguyen, and Marten van Dijk. Buzz: Buffer zones for defending adversarial examples in image classification, 2020.</span></p>\n","categories":["对抗攻击"],"tags":["对抗攻击"]},{"title":"Towards Transferable Adversarial Attacks on Vision Transformers","url":"/posts/4b52/","content":"<h2 id=\"towards-transferable-adversarial-attacks-on-vision-transformers\">Towards Transferable Adversarial Attacks on Vision Transformers</h2>\n<p>目前是关于transformer迁移攻击sota，发表在AAAI 2022，代码链接：https://github.com/zhipeng-wei/PNA-PatchOut</p>\n<span id=\"more\"></span>\n<p>对于transformer的攻击要同时考虑patch和self-attention，本文提出 Pay No Attention (PNA) 攻击和 PatchOut 攻击，在反向传播时跳过注意力可以提高迁移性，同时攻击最优的一部分patch比攻击所有的patch效果好。实验证明所提方法可以提高ViTs之间的迁移和ViTs到CNNs的迁移。</p>\n<p>PNA攻击固定attention前向计算数值，在反向传播时跳过这一部分梯度的计算，这阻止了不同patch之间的交互，图像不同区域之间的交互强度和迁移性是负相关的[<a href=\"#1\">1</a>]。</p>\n<p>PatchOut 攻击在每一次迭代随机选择一部分patch攻击，有点像drop-out缓解过拟合，这和随机森林随机选择特征和DIM攻击相似。</p>\n<p>本文主要贡献点</p>\n<ul>\n<li>PAN攻击不经过对attention的反向传播构造对抗样本。</li>\n<li>研究扰动随机patch子集可以提高迁移性，提出PatchOut 方法在每轮迭代时使用不同的patch作为输入。</li>\n<li>4个不同白盒ViTs，8个黑盒ViTs，4个黑盒CNNs，3个鲁棒训练CNNs进行实验，两种攻击方法有效并且可以和其他方法结合。</li>\n</ul>\n<p>对于第一二点贡献进行了两个toy experiments，ViT-B/16白盒模型的BIM攻击（<span class=\"math inline\">\\(L_{\\infty}\\)</span>范数下，迭代10次，<span class=\"math inline\">\\(\\epsilon = 16\\)</span>），PNA攻击，将12个Self-attention Block分为3组，反向传播时就有八条路线，攻击成功率见下图</p>\n<p><img src=\"/posts/4b52/Users/502/AppData/Roaming/Typora/typora-user-images/image-20220428184941701.png\" alt=\"image-20220428184941701\" style=\"zoom:80%;\"></p>\n<p>PatchOut 攻击随机选取10个patchs作为一种输入模式，记为“ten-patch”，进行多次“ten-patch”攻击成功率高</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/28/PX5Tle2pUEBJoNH.png\" alt=\"image-20220428185342850\"><figcaption aria-hidden=\"true\">image-20220428185342850</figcaption>\n</figure>\n<h3 id=\"方法\">方法</h3>\n<h4 id=\"pay-no-attention-pna\">Pay No Attention (PNA)</h4>\n<p>给定一个patch embedding <span class=\"math inline\">\\(Z\\in \\mathbb{R}^{N\\times D}\\)</span>，query，key，和value 权重<span class=\"math inline\">\\(W^{Q}\\)</span>，<span class=\"math inline\">\\(W^{K}\\)</span>，<span class=\"math inline\">\\(W^{V}\\)</span><span class=\"math inline\">\\(\\in\\)</span> <span class=\"math inline\">\\(\\mathbb{R}^{D\\times D_h}\\)</span>，attention计算如下 <span class=\"math display\">\\[\nA=\\operatorname{softmax}\\left(Z W^{Q}\\left(Z W^{K}\\right)^{T} / \\sqrt{D_{h}}\\right)\n\\]</span> <span class=\"math inline\">\\(A\\in\\mathbb{R}^{N\\times N}\\)</span>表示注意力权重，一个head的输出定义如下 <span class=\"math display\">\\[\nZ^{\\prime}=A\\left(Z W^{V}\\right)\n\\]</span> 输出<span class=\"math inline\">\\(Z^{\\prime}\\)</span>对于输入<span class=\"math inline\">\\(Z\\)</span>的梯度如下 <span class=\"math display\">\\[\n\\frac{\\partial Z^{\\prime}}{\\partial Z}=(\\mathbb{I} \\bigotimes A) \\frac{\\partial\\left(Z W^{V}\\right)}{\\partial Z}+\\left(\\left(Z W^{V}\\right)^{T} \\bigotimes \\mathbb{I}\\right) \\frac{\\partial A}{\\partial Z}\n\\]</span> PNA方法忽视注意力部分的梯度，<span class=\"math inline\">\\(\\frac{\\partial A}{\\partial Z}=0\\)</span>，这相当于将attention weights固定为一个常数。最终梯度计算如下 <span class=\"math display\">\\[\n\\frac{\\partial Z^{\\prime}}{\\partial Z} \\approx(\\mathbb{I} \\bigotimes A) \\frac{\\partial\\left(Z W^{V}\\right)}{\\partial Z}=(\\mathbb{I} \\bigotimes A)\\left(\\left(W^{V}\\right)^{T} \\bigotimes \\mathbb{I}\\right)\n\\]</span> <span class=\"math inline\">\\(\\bigotimes\\)</span>表示克罗内克积，PNA迫使扰动只通过使用特征表征来利用网络，而不是通过利用注意力的高度模型特定属性。这导致了具有高迁移性的对抗样本。跳过注意力也允许梯度关注每个patch，而不是依赖patch之间的复杂相互作用。</p>\n<h4 id=\"patchout-attack\">PatchOut Attack</h4>\n<p>DIM攻击表明输入多样性有利于对抗向本迁移，PatchOut Attack每次迭代随机选择patch子集攻击。</p>\n<p>用<span class=\"math inline\">\\(T\\)</span>控制每一轮使用的patch数量，<span class=\"math inline\">\\(x_s=\\{x_s^{1},...,x_s^{t},...,x_s^{T}\\}\\)</span>表示被选择的patch，攻击的mask <span class=\"math inline\">\\(M\\in {0,1}^{H\\times W\\times C}\\)</span> 定义如下 <span class=\"math display\">\\[\nM_{p}^{i}= \\begin{cases}1, &amp; \\text { if } x_{p}^{i} \\text { in } x_{s} \\\\ 0, &amp; \\text { otherwise }\\end{cases}\n\\]</span> <span class=\"math inline\">\\(M_{p}^{i} \\in\\{0,1\\}^{P \\times P \\times C}\\)</span>是<span class=\"math inline\">\\(x_p ^{i}\\)</span>在图像中区域，整个攻击的优化目标 <span class=\"math display\">\\[\n\\underset{\\delta}{\\arg \\max } J(f(x+M \\odot \\delta), y)+\\lambda\\|\\delta\\|_{2}$, s.t. $\\|\\delta\\|_{\\infty}&lt;\\epsilon\n\\]</span> <span class=\"math inline\">\\(\\odot\\)</span>是element-wise乘法，第二项鼓励扰动有一个较大的<span class=\"math inline\">\\(l_2\\)</span>范数，两种攻击的伪代码如下</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/28/ufKO3mzARTbcg1U.png\" alt=\"image-20220428204122682\"><figcaption aria-hidden=\"true\">image-20220428204122682</figcaption>\n</figure>\n<h3 id=\"实验结果\">实验结果</h3>\n<p>在CNNS上的效果</p>\n<p>数据集：ImageNet val中1000张不同类所有模型正确分类图像，白盒模型 ViT-B/16，PiT-B，CaiT-S-24，Visformer-S，图中每个数据都是四种替代模型迁移攻击成功率的平均。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/28/YAbBRV5lyvcsGFQ.png\" alt=\"image-20220428204253145\"><figcaption aria-hidden=\"true\">image-20220428204253145</figcaption>\n</figure>\n<p>本文方法和之前方法的结合，白盒模型ViT-B/16，表中是平均迁移攻击成功率，8个ViTs：ViT-B/16，DeiT-B，TNT-S，LeViT-256，PiT-B ，CaiT-S-24，ConViT-B 和 Visformer-S，4个CNNs：Inception v3 (Inc-v3)，Inception v4 (Incv4)，Inception ResNet v2 (IncRes-v2)， 和 ResNet v2-152 (Res-v2)。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/28/T8h9uQrvJz6SO2y.png\" alt=\"image-20220428204936007\"><figcaption aria-hidden=\"true\">image-20220428204936007</figcaption>\n</figure>\n<h4 id=\"消融实验\">消融实验</h4>\n<p>白盒模型 ViT-B/16，数据 2000张随机采样图像，平均迁移攻击成功率</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/28/IHqF9fCJXsyNVGn.png\" alt=\"image-20220428210344909\"><figcaption aria-hidden=\"true\">image-20220428210344909</figcaption>\n</figure>\n<p>三部分都有效，一起使用效果最佳</p>\n<p>超参数影响，ViTs 平均攻击成功率 <span class=\"math inline\">\\(T=N\\)</span> PatchOut 退化成 BIM，</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/04/28/s8qo4Hv6x9F7TBl.png\" alt=\"image-20220428210530488\"><figcaption aria-hidden=\"true\">image-20220428210530488</figcaption>\n</figure>\n<p><span id=\"1\">[1] Wang, X.; Ren, J.; Lin, S.; Zhu, X.;Wang, Y.; and Zhang, Q.2020. A unified approach to interpreting and boosting adversarial transferability. arXiv preprint arXiv:2010.04055.</span></p>\n","categories":["攻击"]},{"title":"No-box Attack","url":"/posts/5b1c/","content":"<h1 id=\"practical-no-box-adversarial-attacks-against-dnns\">Practical No-box Adversarial Attacks against DNNs</h1>\n<p>发表在 NeurIPS 2020，代码：https://github.com/qizhangli/nobox-attacks</p>\n<span id=\"more\"></span>\n<p>提出 no box 设定：禁止查询目标模型，模型结构参数和训练数据也未知，攻击者只能收集来自相似任务的少量数据（几十张或者更少）。依靠这些少量的数据进行有监督的训练得到一个替代模型十分困难。</p>\n<p>本文使用 auto-encoders 从少量数据（来自两个类别的20张图片）学习判别性的特征，并研究了三种训练机制：a) 估计每个旋转图像的正面视图，b) 估计每个可能的拼图的最佳拟合，c) 构建原型图像。auto-encoding 模型生成的对抗样本可以很好的迁移到不同的目标模型上，它的有效性有时与使用在与目标模型相同的大规模数据集上训练的预训练模型构建的样本不相上下。</p>\n<h2 id=\"方法\">方法</h2>\n<h3 id=\"训练方法\">训练方法</h3>\n<p>本文针对非目标攻击，训练集 <span class=\"math inline\">\\(\\mathcal{X}:=\\{(x_i,y_i)\\}_{i=0}^{n-1}\\)</span> ，本文考虑这个辅助数据集一共有两类，i.e.，<span class=\"math inline\">\\(y_i\\in \\{0,1\\}\\)</span>。</p>\n<p>Baseline 借助于图像到图像的映射，它们在建模数据的内部分布方面是成功的，最小化<span class=\"math inline\">\\(\\sum_i \\Vert\\text{Dec}(\\text{Enc}(x_i))-x_i \\Vert^2\\)</span>， 这样的模型一方面能够捕获低层次的图像表示，不会遭受严重的过拟合，另一方面，模型没有判别能力，因此针对模型生成的对样本例很难迁移到其他模型上。</p>\n<p><strong>Reconstruction from chaos</strong> 已有研究表明，预测图像旋转角度和拼图构型赋予DNNs相当大的判别能力，使用image-to-image auto-encoding 模型，结合上述辅助信息，引入评估任务：(a) 每个旋转图像的前视图和 (b) 每一个可能的拼图的完美配合，学习目标函数： <span class=\"math display\">\\[\nL_{\\text{rotaion/jigsaw}}=\\frac{1}{n}\\sum_{i=0}^{n-1}\\Vert \\text{Dec}(\\text{Enc}(T(x_i)))-x_i\\Vert^2\n\\]</span></p>\n<p>上式图像转换函数<span class=\"math inline\">\\(T(\\cdot)\\)</span> 对于这两个任务分别是旋转图像和随机打乱图像patches，虽然学习任务仍然像经典的自动编码器一样是无监督，但辅助信息是与内容相关的，应该有利于分类。</p>\n<p><strong>Prototypical image reconstruction</strong> 利用标签信息，鼓励模型重建特定类别原型， <span class=\"math display\">\\[\nL_{\\text {prototypical}}=\\frac{1}{n} \\sum_{i=0}^{n-1}\\left(\\left(1-y_i\\right)\\left\\|\\text{Dec}\\left(\\text{Enc}\\left(x_i\\right)\\right)-\nx^{(0)}\\right\\|^2+y_i\\left\\|\\text{Dec}\\left(\\text{Enc}\\left(x_i\\right)\\right)-x^{(1)}\\right\\|^2\\right)\n\\]</span> 上式<span class=\"math inline\">\\(x^0 \\in \\{x_i|y_i=0\\}\\)</span>，<span class=\"math inline\">\\(x^1 \\in \\{x_i|y_i=1\\}\\)</span>是随机选择的类别原型，这种机制背后的直觉是，一个模型将必须区分具有不同标签的样本，以获得完美的训练</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/11/12/lWj9aoREbqxUiCc.png\" alt=\"image-20221112205641760\"><figcaption aria-hidden=\"true\">image-20221112205641760</figcaption>\n</figure>\n<h3 id=\"提高攻击迁移性\">提高攻击迁移性</h3>\n<p>定义 auto-encoding 模型的对抗损失，这个损失用来训练模型，对每个<span class=\"math inline\">\\(x_i\\)</span> 重建 <span class=\"math inline\">\\(\\tilde{x}_i\\)</span>。 <span class=\"math display\">\\[\nL_{\\text {adversarial }}=-\\log p\\left(y_i \\mid x_i\\right) \\quad \\text { where } \\quad p\\left(y_i \\mid x_i\\right)=\\frac{\\exp \\left(-\\lambda\\left\\|\\text{Dec}\\left(\\text{Enc}\\left(x_i\\right)\\right)-\\tilde{x}_i\\right\\|^2\\right)}{\\sum_j \\exp \\left(-\\lambda\\left\\|\\text{Dec}\\left(\\text{Enc}\\left(x_i\\right)\\right)-\\tilde{x}_j\\right\\|^2\\right)}\n\\]</span> <span class=\"math inline\">\\(\\lambda&gt;0\\)</span>，对于旋转和拼图，<span class=\"math inline\">\\(\\tilde{x}_i:=x\\)</span>，对于原型重建，当 <span class=\"math inline\">\\(x_i\\)</span> 的标签 <span class=\"math inline\">\\(y_i=0\\)</span>，<span class=\"math inline\">\\(\\tilde{x}_i:=x^{(0)}\\)</span>。这样的<span class=\"math inline\">\\(\\tilde{x}_i\\)</span>被称为 <span class=\"math inline\">\\(x_i\\)</span> 的 positive prototype，通过攻击，他们的距离增大，对抗损失<span class=\"math inline\">\\(L_{\\text{adversarial}}\\)</span>最大化。在本文中使用ILA攻击方法，首先使用一个基于梯度的攻击方法获得一个攻击扰动，然后用编码器的输出进行ILA攻击。</p>\n<h2 id=\"实验\">实验</h2>\n<p><strong>设定</strong> 分类任务：从 ImageNet 随机选择5000张（500类，一类10张），<span class=\"math inline\">\\(\\epsilon = 0.1\\)</span>或者<span class=\"math inline\">\\(0.8\\)</span>。人脸验证任务：LFW数据集，随机选取来自400个个体2110张图，<span class=\"math inline\">\\(\\epsilon = 0.1\\)</span>，攻击一个商用系统 clarifai.com。图片resize为<span class=\"math inline\">\\(112\\times112\\)</span>。</p>\n<p><strong>方法</strong> 对于rotation，考虑四个选择角度：0°，90°，180°，270°，对于jigsaw，原始图形平均分为四部分随机打乱，优化器adam，学习率0.001，最大训练次数15000，一个替代模型训练完毕后，I-FGSM攻击200步，然后ILA 100步，<span class=\"math inline\">\\(\\lambda = 1\\)</span>，对于ImageNet和LFW，攻击步长1/255.</p>\n<p><strong>Competitors</strong> 本文提出两个baseline，<span class=\"math inline\">\\(naïve^{\\dagger}\\)</span>：监督训练的ResNets，正则化和数据增广训练得到，<span class=\"math inline\">\\(naïve^{\\ddagger}\\)</span>：无监督训练的 auto-encoders，<span class=\"math inline\">\\(naïve^{\\dagger}\\)</span>和<span class=\"math inline\">\\(naïve^{\\ddagger}\\)</span>训练数据不超过20张图像，<span class=\"math inline\">\\(Beyonder\\)</span>：在大规模数据集预训练的模型，训练数据和目标模型一致，在本文中选取ResNet-50</p>\n<p><strong>实验结果</strong> 对于prototypical机制测试单个解码器和多个解码器的模型，无监督训练的模型rotation最优，</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/11/13/Eby2HjlkUP4WItT.png\" alt=\"image-20221113215728529\"><figcaption aria-hidden=\"true\">image-20221113215728529</figcaption>\n</figure>\n<h1 id=\"adversarial-pixel-restoration-as-a-pretext-task-for-transferable-perturbations\">Adversarial Pixel Restoration as a Pretext Task for Transferable Perturbations</h1>\n<p>发表在 BMVC 2022，代码：https://github.com/HashmatShadab/APR</p>\n<p>设定和上一篇相同，在上一篇工作中prototypical方法需要有监督训练，为了减小对少量样本数据的过拟合，并且找到鲁棒的特征，提出对抗像素复原（Adversarial Pixel Restoration）自监督任务，用少量无标签样本训练替代模型。本文提出的min-max目标函数可以找到一个更鲁棒更平坦的最小值，提高对抗迁移性。</p>\n<p>本文3点贡献：1. 提出了一种自监督对抗像素恢复方法，通过在平坦损失表面上学习来找到高度可转移的模式。我们的训练方法允许在不访问大规模标记数据或预训练模型的情况下实现cross-domain 攻击。2. 对抗攻击方法是自监督的，和任务无关。所提方法生成的扰动可以迁移到不同任务中。3. 本文方法使损失更平滑，有助于生成更通用的对抗样本。当训练集仅包含两个样本时方法依然有效。</p>\n<h2 id=\"方法-1\">方法</h2>\n<p>给定一个无标签的数据分布<span class=\"math inline\">\\(p_s\\)</span>，样本数量<span class=\"math inline\">\\(\\leq 20\\)</span>，由于数据都是无标签的，可以定义一个自监督学习任务<span class=\"math inline\">\\(\\mathcal{T}_s\\)</span>训练替代模型，一个主要的挑战是，深度神经网络可以很容易地记忆数据，即使使用较强的数据增广方法，模型还是会快速过拟合少数可用的样本。这导致替代模型的可泛化性较差，因此，从该模型生成的对抗攻击具有较弱的可转移性。</p>\n<p><strong>Adversarial Training via Denoising</strong> 本文提出一个min-max训练过程，max过程通过单步攻击的对抗像素变换欺骗模型<span class=\"math inline\">\\(\\mathcal{F}\\)</span>，min过程去噪并恢复特征和像素空间，实现可泛化的损失面。</p>\n<p><strong>对抗像素变换</strong> 对于输入<span class=\"math inline\">\\(x \\sim P_s\\)</span>，通过下面目标寻找满足<span class=\"math inline\">\\(\\Vert x-x^{\\prime}\\Vert_{\\infty} \\leq \\varepsilon\\)</span>对抗样本<span class=\"math inline\">\\(x^{\\prime}\\)</span>， <span class=\"math display\">\\[\n\\underset{\\boldsymbol{x}^{\\prime}}{\\operatorname{maximize}} \\quad \\mathcal{L}_{\\max }=\\left\\|\\mathcal{F}\\left(\\mathcal{T}_s\\left(\\boldsymbol{x}^{\\prime}\\right)\\right)-\\boldsymbol{x}\\right\\|^p \\tag{1}\n\\]</span> <span class=\"math inline\">\\(\\mathcal{T}_s\\)</span>代表像素变换，例如rotation或者jigsaw shuffle，因此，我们的攻击通过最大化式5中所示的损失来欺骗模型恢复转换像素空间的能力，并最终帮助鲁棒自监督特征。上述攻击方法扩展到有监督的情况： <span class=\"math display\">\\[\n\\underset{\\boldsymbol{x}^{\\prime}}{\\operatorname{maximize}} \\sum_{c=1}^C\\left(y_c\\left\\|\\mathcal{F}\\left(\\mathcal{T}_s\\left(\\boldsymbol{x}^{\\prime}\\right)\\right)-\\boldsymbol{x}^{(c)}\\right\\|^p\\right) \\tag{2}\n\\]</span> 其中<span class=\"math inline\">\\(C\\)</span>是类别数，<span class=\"math inline\">\\(y_c\\)</span>是one-hot标签，<span class=\"math inline\">\\(x^{(c)}\\)</span>是选取的特定类别原型。</p>\n<p><strong>像素重建</strong> 对于公式1生成的对抗样本<span class=\"math inline\">\\(x^{\\prime}\\)</span>，最小化原始图像<span class=\"math inline\">\\(x\\)</span>和模型对抗变换输出以及原始样本变换输出之间的重建错误： <span class=\"math display\">\\[\n\\mathcal{L}_{\\text {out }}=\\left\\|\\mathcal{F}\\left(\\mathcal{T}_s\\left(\\boldsymbol{x}^{\\prime}\\right)\\right)-\\boldsymbol{x}\\right\\|^p+\\left\\|\\mathcal{F}\\left(\\mathcal{T}_s(\\boldsymbol{x})\\right)-\\boldsymbol{x}\\right\\|^p \\tag{3}\n\\]</span> 在对抗训练中，我们进一步调节模型的特征空间，通过强制原始和对抗特征分布之间的对齐，如下所示： <span class=\"math display\">\\[\n\\mathcal{L}_{\\text {feature }}=\\left\\|\\mathcal{F}^n\\left(\\mathcal{T}_s\\left(\\boldsymbol{x}^{\\prime}\\right)\\right)-\\mathcal{F}^n\\left(\\mathcal{T}_s(\\boldsymbol{x})\\right)\\right\\|^p \\tag{4}\n\\]</span> 其中<span class=\"math inline\">\\(\\mathcal{F}^n\\)</span>代表encoder中间层输出，整个训练目标： <span class=\"math display\">\\[\n\\mathcal{L}_{\\text {min }}=\\mathcal{L}_{\\text {out }}+\\lambda \\mathcal{L}_{\\text {feature }} \\tag{5}\n\\]</span> 训练过程伪代码：</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/11/14/S816zNAhk4ECIPv.png\" alt=\"image-20221114195358774\"><figcaption aria-hidden=\"true\">image-20221114195358774</figcaption>\n</figure>\n<h2 id=\"实验-1\">实验</h2>\n<p>本文考虑<span class=\"math inline\">\\(x_{\\infty}\\)</span>对抗攻击，扰动边界<span class=\"math inline\">\\(\\varepsilon \\leq 0.1\\)</span>，考虑in-domian和cross-domain数据，adam优化器学习率0.001，<span class=\"math inline\">\\(\\delta=\\frac{2}{255},\\lambda=1,p=2\\)</span>。</p>\n<p><strong>Surrogate Training with Few Samples:</strong> 使用公式1作为无监督对抗像素变换（<span class=\"math inline\">\\(\\mathcal{T}\\)</span>）,公式2针对有标签，使用多个decoder的模型。</p>\n<p><strong>Surrogate Training with Large Dataset:</strong> 在paintings（79K 样本），CoCo（40K 样本），和Comics（50K 样本）训练替代模型，测试本文所提方法的cross-domain对抗迁移性。</p>\n<p><strong>目标模型：</strong> VGG-19，Inc-V3，Res152，Dense161，SeNet，WRN，MNet-V2，ViT-S，DeiT-T 和DeiT-S，鲁棒ResNet-50，DETR 和DINO评估跨任务迁移性。</p>\n<p><strong>数据集：</strong>在5K 来自ImageNet Val样本测试分类模型，目标检测CoCo（5K）视频分割DAVIS （2K）</p>\n<p><strong>Baseline 对抗攻击：</strong>200 iterations I-FGSM，100 iterations of ILA，攻击目标：公式1</p>\n<p>in-domain</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/11/14/k29vzrt1N3BYCHX.png\" alt=\"image-20221114204819477\"><figcaption aria-hidden=\"true\">image-20221114204819477</figcaption>\n</figure>\n<p>cross-domain：在所有CNNs上求平均的结果</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/11/14/kBcXtZC9fJjF52s.png\" alt=\"image-20221114204952181\"><figcaption aria-hidden=\"true\">image-20221114204952181</figcaption>\n</figure>\n<p>消融实验结果省略......</p>\n<h1 id=\"practical-no-box-adversarial-attacks-with-training-free-hybrid-image-transformation\">Practical No-box Adversarial Attacks with Training-free Hybrid Image Transformation</h1>\n<p>还未正式发表</p>\n<h2 id=\"动机\">动机</h2>\n<p>针对no-box提出一种不需要训练的方法，在分类任务中低层网络提取到的特征的高频部分（HFC）至关重要，如果一个扰动可以有效地控制图像的HFC，模型将会提取完全不同的底层特征，导致模型错分。[<a href=\"#id2\">2</a>]表明对抗扰动的有效性在于它包含不相关的特征，这些扰动的特征在原始图像特征中占主导地位，文本设计包含不相关特征的的对抗HFC，本文发现有效噪声HFC是区域均匀，重复密集的。</p>\n<h2 id=\"方法-2\">方法</h2>\n<p>本文猜测有效的扰动应该是均匀重复密集的，采用混合图像（hybrid image [<a href=\"#id1\">1</a>]）的思想，提出一种 Hybrid Image Transformation (HIT) 攻击方法。hybrid image：将一张图像的HFC替换为另一张精心挑选的图像的HFC，并用两种不同的解释制作混合图像:一种是在近距离观看图像时出现的，另一种是从远处出现的。HIT 减少了原始的HFC，同时增加了精心设计的噪声HFC来攻击dnn。</p>\n<p>具体实现就是生成由下面三种图案均匀排列的patch <span class=\"math inline\">\\(x^p\\)</span>，这个patch的大小和要攻击的图像相同，一个图案包括几个形状定义为密集度</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/11/17/FezKWilNa87L4G1.png\" alt=\"image-20221117210037610\"><figcaption aria-hidden=\"true\">image-20221117210037610</figcaption>\n</figure>\n<p>然后定义一个高斯低通滤波器<span class=\"math inline\">\\(G\\)</span>，大小是（<span class=\"math inline\">\\(4k+1 \\times 4k+1\\)</span> ）,定义为： <span class=\"math display\">\\[\nG_{i,j}=\\frac{1}{2\\pi \\sigma^2}e^{-(\\frac{i^2+j^2}{2\\sigma^2})} \\tag{1}\n\\]</span> <span class=\"math inline\">\\(\\sigma\\)</span>越大，更多的HFC将被过滤掉，用原始图像的低频部分和patch的高频部分融合生成对抗样本： <span class=\"math display\">\\[\nx^{a d v}=\\operatorname{clip}_{x, \\varepsilon}\\left(x * G+\\lambda \\cdot\\left(x^p-x^p * G\\right)\\right) \\tag{2}\n\\]</span> 从下图可以直观的看到，黄框框出来的box，HIT减小相关的HFC并且添加了不相关的噪声，目标模型无法提取正确的特征来做出合理的预测，从而导致分类错误</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/11/17/m24vPhSbnFq7lIL.png\" alt=\"image-20221117211822937\"><figcaption aria-hidden=\"true\">image-20221117211822937</figcaption>\n</figure>\n<h2 id=\"实验-2\">实验</h2>\n<p>实验部分考虑了ImageNet和三个DCL 框架[<a href=\"#id3\">3</a>]下细粒度分类CUB-200-2011，Stanford Cars 和 FGVC Aircraft，由圆组成的patch攻击效果更好，实验结果和消融实验具体看原本，这里就不放了</p>\n<p><a id=\"id1\">[1] Aude Oliva. The art of hybrid images: Two for the view of one. Art &amp; Perception, 1(1-2):65–74, 2013.</a></p>\n<p><a id=\"id2\">[2] Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In-So Kweon. Understanding adversarial examples from the mutual influence of images and perturbations. In CVPR, 2020.</a>&gt;</p>\n<p><a id=\"id3\">[3] Yue Chen, Yalong Bai,Wei Zhang, and Tao Mei. Destruction and construction learning for fine-grained image recognition. In CVPR, 2019.</a></p>\n<h1 id=\"总结\">总结</h1>\n<p>这两篇工作考虑了实际中训练替代模型和目标模型的数据分布不一致，且仅有少量训练数据（有标签/无标签）可用是如何训练替代模型。但是在实验设定上用20个样本训练一个替代模型，攻击5000个样本需要训练250个替代模型，在实验设置上并没有对比用5000个样本训练一个模型的迁移效果。在第一篇中Beyonder作为替代模型测试IFGSM对5000样本攻击迁移效果，其他方法都是IFGSM+ILA，对于不太公平。IFGSM和ILA攻击收敛慢，在文章中分别要迭代200和100步，减少迭代次数达不到最佳效果。</p>\n<p>在第三个方法中，HIT用在某些中间层feature上效果如何还可进一步研究。图案的选取没有什么依据，在文章中对比了三种最常见的形状，对抗样本的生成不需要训练，也不需要迭代，从这一点来看是一种非深度学习的方法。</p>\n","categories":["攻击","自监督"],"tags":["对抗攻击"]},{"title":"一些packages","url":"/posts/6731/","content":"<h2 id=\"timm\">timm</h2>\n<p>官方文档</p>\n<p><a href=\"https://rwightman.github.io/pytorch-image-models/models/\">Model Summaries - Pytorch Image Models (rwightman.github.io)</a></p>\n<p>安装</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\">pip install timm</span><br></pre></td></tr></tbody></table></figure>\n<span id=\"more\"></span>\n<p>建立模型</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> timm</span><br><span class=\"line\"></span><br><span class=\"line\">m = timm.create_model(<span class=\"string\">'mobilenetv3_large_100'</span>, pretrained=<span class=\"literal\">True</span>)</span><br><span class=\"line\">m.<span class=\"built_in\">eval</span>()</span><br></pre></td></tr></tbody></table></figure>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> timm</span><br><span class=\"line\"><span class=\"keyword\">from</span> pprint <span class=\"keyword\">import</span> pprint</span><br><span class=\"line\">model_names = timm.list_models(pretrained=<span class=\"literal\">True</span>)</span><br><span class=\"line\">pprint(model_names)</span><br></pre></td></tr></tbody></table></figure>\n<ol start=\"2\" type=\"1\">\n<li>模块调用</li>\n</ol>\n<p><a href=\"https://rwightman.github.io/pytorch-image-models/training_hparam_examples/\">Training Examples - Pytorch Image Models (rwightman.github.io)</a></p>\n<p><a href=\"https://rwightman.github.io/pytorch-image-models/feature_extraction/\">Feature Extraction - Pytorch Image Models (rwightman.github.io)</a></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> timm</span><br><span class=\"line\">m = timm.create_model(<span class=\"string\">'resnet50'</span>, pretrained=<span class=\"literal\">True</span>, num_classes=<span class=\"number\">0</span>, global_pool=<span class=\"string\">''</span>)</span><br><span class=\"line\">o = m(torch.randn(<span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f'Unpooled shape: <span class=\"subst\">{o.shape}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure>\n<h2 id=\"pretrainedmodels\">pretrainedmodels</h2>\n<p>https://github.com/Cadene/pretrained-models.pytorch</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pretrainedmodels</span><br><span class=\"line\"><span class=\"built_in\">print</span>(pretrainedmodels.model_names)</span><br><span class=\"line\">&gt; [<span class=\"string\">'fbresnet152'</span>, <span class=\"string\">'bninception'</span>, <span class=\"string\">'resnext101_32x4d'</span>, <span class=\"string\">'resnext101_64x4d'</span>, <span class=\"string\">'inceptionv4'</span>, <span class=\"string\">'inceptionresnetv2'</span>, <span class=\"string\">'alexnet'</span>, <span class=\"string\">'densenet121'</span>, <span class=\"string\">'densenet169'</span>, <span class=\"string\">'densenet201'</span>, <span class=\"string\">'densenet161'</span>, <span class=\"string\">'resnet18'</span>, <span class=\"string\">'resnet34'</span>, <span class=\"string\">'resnet50'</span>, <span class=\"string\">'resnet101'</span>, <span class=\"string\">'resnet152'</span>, <span class=\"string\">'inceptionv3'</span>, <span class=\"string\">'squeezenet1_0'</span>, <span class=\"string\">'squeezenet1_1'</span>, <span class=\"string\">'vgg11'</span>, <span class=\"string\">'vgg11_bn'</span>, <span class=\"string\">'vgg13'</span>, <span class=\"string\">'vgg13_bn'</span>, <span class=\"string\">'vgg16'</span>, <span class=\"string\">'vgg16_bn'</span>, <span class=\"string\">'vgg19_bn'</span>, <span class=\"string\">'vgg19'</span>, <span class=\"string\">'nasnetalarge'</span>, <span class=\"string\">'nasnetamobile'</span>, <span class=\"string\">'cafferesnet101'</span>, <span class=\"string\">'senet154'</span>,  <span class=\"string\">'se_resnet50'</span>, <span class=\"string\">'se_resnet101'</span>, <span class=\"string\">'se_resnet152'</span>, <span class=\"string\">'se_resnext50_32x4d'</span>, <span class=\"string\">'se_resnext101_32x4d'</span>, <span class=\"string\">'cafferesnet101'</span>, <span class=\"string\">'polynet'</span>, <span class=\"string\">'pnasnet5large'</span>]</span><br></pre></td></tr></tbody></table></figure>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(pretrainedmodels.pretrained_settings[<span class=\"string\">'nasnetalarge'</span>])</span><br><span class=\"line\">&gt; {<span class=\"string\">'imagenet'</span>: {<span class=\"string\">'url'</span>: <span class=\"string\">'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'</span>, <span class=\"string\">'input_space'</span>: <span class=\"string\">'RGB'</span>, <span class=\"string\">'input_size'</span>: [<span class=\"number\">3</span>, <span class=\"number\">331</span>, <span class=\"number\">331</span>], <span class=\"string\">'input_range'</span>: [<span class=\"number\">0</span>, <span class=\"number\">1</span>], <span class=\"string\">'mean'</span>: [<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>], <span class=\"string\">'std'</span>: [<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>], <span class=\"string\">'num_classes'</span>: <span class=\"number\">1000</span>}, <span class=\"string\">'imagenet+background'</span>: {<span class=\"string\">'url'</span>: <span class=\"string\">'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth'</span>, <span class=\"string\">'input_space'</span>: <span class=\"string\">'RGB'</span>, <span class=\"string\">'input_size'</span>: [<span class=\"number\">3</span>, <span class=\"number\">331</span>, <span class=\"number\">331</span>], <span class=\"string\">'input_range'</span>: [<span class=\"number\">0</span>, <span class=\"number\">1</span>], <span class=\"string\">'mean'</span>: [<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>], <span class=\"string\">'std'</span>: [<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>], <span class=\"string\">'num_classes'</span>: <span class=\"number\">1001</span>}}</span><br></pre></td></tr></tbody></table></figure>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\">model_name = <span class=\"string\">'nasnetalarge'</span> <span class=\"comment\"># could be fbresnet152 or inceptionresnetv2</span></span><br><span class=\"line\">model = pretrainedmodels.__dict__[model_name](num_classes=<span class=\"number\">1000</span>, pretrained=<span class=\"string\">'imagenet'</span>)</span><br><span class=\"line\">model.<span class=\"built_in\">eval</span>()</span><br></pre></td></tr></tbody></table></figure>\n<h2 id=\"各种vit\">各种ViT</h2>\n<p>https://github.com/lucidrains/vit-pytorch</p>\n<p>专栏：https://www.zhihu.com/column/c_1260183218026213376</p>\n<h2 id=\"torchsummary可视化\">torchsummary可视化</h2>\n<ol type=\"1\">\n<li>安装</li>\n</ol>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\">pip install torchsummary</span><br></pre></td></tr></tbody></table></figure>\n<ol start=\"2\" type=\"1\">\n<li>使用方法</li>\n</ol>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch, torchvision</span><br><span class=\"line\">model = torchvision.models.vgg</span><br><span class=\"line\">model = torchvision.models.vgg16()</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchsummary <span class=\"keyword\">import</span> summary</span><br><span class=\"line\">summary(model, (<span class=\"number\">3</span>, <span class=\"number\">224</span>, <span class=\"number\">224</span>))</span><br></pre></td></tr></tbody></table></figure>\n<h2 id=\"可视化\">可视化</h2>\n<p>https://github.com/utkuozbulak/pytorch-cnn-visualizations</p>\n<p>https://github.com/JonnesLin/Evison</p>\n<p>https://www.jianshu.com/p/0431d6d89d8e</p>\n<p>CAM可视化：</p>\n<p>https://github.com/jacobgil/pytorch-grad-cam</p>\n<p>t-SNE可视化</p>\n<p><a href=\"https://github.com/DmitryUlyanov/Multicore-TSNE\">DmitryUlyanov/Multicore-TSNE: Parallel t-SNE implementation with Python and Torch wrappers. (github.com)</a></p>\n<p><a href=\"https://github.com/CannyLab/tsne-cuda\">CannyLab/tsne-cuda: GPU Accelerated t-SNE for CUDA with Python bindings (github.com)</a></p>\n<h3 id=\"优雅地操作张量维度\">优雅地操作张量维度</h3>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\">pip install einops</span><br></pre></td></tr></tbody></table></figure>\n<p><a href=\"https://zhuanlan.zhihu.com/p/342675997\">PyTorch 70.einops：优雅地操作张量维度 - 知乎 (zhihu.com)</a></p>\n<h3 id=\"可微cv库\">可微cv库</h3>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\">pip install Kornia</span><br></pre></td></tr></tbody></table></figure>\n<p>官方文档</p>\n<p>https://kornia.readthedocs.io/en/latest/index.html</p>\n<h2 id=\"其它\">其它</h2>\n<h3 id=\"基于-pytorch-的开源-图像视频复原工具箱-比如-超分辨率-去噪-去模糊-去-jpeg-压缩噪声等.\">基于 PyTorch 的开源 图像视频复原工具箱, 比如 超分辨率, 去噪, 去模糊, 去 JPEG 压缩噪声等.</h3>\n<p>https://github.com/xinntao/BasicSR</p>\n<h3 id=\"去雨去噪去模糊\">去雨去噪去模糊</h3>\n<p><a href=\"https://github.com/hpcaitech/ColossalAI\">hpcaitech/ColossalAI: Colossal-AI: A Unified Deep Learning System for Large-Scale Parallel Training (github.com)</a></p>\n<h3 id=\"用于模型训练\">用于模型训练</h3>\n<p><a href=\"https://github.com/facebookresearch/fairscale\">fairscale</a></p>\n<p><a href=\"https://github.com/hpcaitech/ColossalAI\">hpcaitech/ColossalAI: Colossal-AI: A Unified Deep Learning System for Large-Scale Parallel Training (github.com)</a></p>\n<h3 id=\"gan\">GAN</h3>\n<p><a href=\"https://github.com/eriklindernoren/PyTorch-GAN\">eriklindernoren/PyTorch-GAN: PyTorch implementations of Generative Adversarial Networks. (github.com)</a></p>\n<h3 id=\"autodl\">AutoDL</h3>\n<p><a href=\"https://github.com/D-X-Y/AutoDL-Projects\">D-X-Y/AutoDL-Projects: Automated deep learning algorithms implemented in PyTorch. (github.com)</a></p>\n<h3 id=\"攻击防御\">攻击&amp;防御</h3>\n<p>https://github.com/thu-ml/ares</p>\n<h3 id=\"调参\">调参</h3>\n<p>https://github.com/optuna/optuna</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/399174504\">用Optuna进行Pytorch超参数调优教程 - 知乎 (zhihu.com)</a></p>\n<p>https://github.com/ray-project/ray</p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/364613087\">AutoML: 自动调参工具-Ray Tune - 知乎 (zhihu.com)</a></p>\n<p><a href=\"https://scikit-optimize.github.io/stable/index.html\">scikit-optimize: sequential model-based optimization in Python — scikit-optimize 0.8.1 documentation</a></p>\n<h3 id=\"ml_collections\">ml_collections</h3>\n<p>ML Collections是为ML use cases而设计的一个Python Collections的一个库。它的两个类是ConfigDict和FrozenConfigDict，是\"dict-like\" 的数据结构，以下是ConfigDict、FrozenConfigDict和FieldReference的示例用法，</p>\n<p>https://blog.csdn.net/weixin_47066348/article/details/113954876</p>\n<p><a href=\"https://ml-collections.readthedocs.io/en/latest/\">Welcome to ml_collections’s documentation! — ml_collections 0.1.0 documentation (ml-collections.readthedocs.io)</a></p>\n<p>https://github.com/google/ml_collections</p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> ml_collections</span><br><span class=\"line\"></span><br><span class=\"line\">cfg = ml_collections.ConfigDict()</span><br><span class=\"line\">cfg.float_field = <span class=\"number\">12.6</span> <span class=\"comment\">#float类型</span></span><br><span class=\"line\">cfg.integer_field = <span class=\"number\">123</span> <span class=\"comment\">#int类型</span></span><br><span class=\"line\">cfg.another_integer_field = <span class=\"number\">234</span> <span class=\"comment\">#int类型</span></span><br><span class=\"line\">cfg.nested = ml_collections.ConfigDict() <span class=\"comment\">#嵌套了ml_collections.ConfigDict()</span></span><br><span class=\"line\">cfg.nested.string_field = <span class=\"string\">'tom'</span> <span class=\"comment\">#str类型</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(cfg.integer_field)  <span class=\"comment\"># 输出结果 123.</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(cfg[<span class=\"string\">'integer_field'</span>])  <span class=\"comment\"># 也输出123.</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">  cfg.integer_field = <span class=\"string\">'tom'</span>  <span class=\"comment\"># 输出会报错错误类型是TypeError，因为这你field是整数类型</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> TypeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(e)</span><br><span class=\"line\"></span><br><span class=\"line\">cfg.float_field = <span class=\"number\">12</span>  <span class=\"comment\"># int类型也可以指定给float类型.</span></span><br><span class=\"line\">cfg.nested.string_field = <span class=\"string\">u'bob'</span>  <span class=\"comment\"># string可以储存Unicode字符串</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(cfg)</span><br><span class=\"line\"><span class=\"comment\">##############################</span></span><br><span class=\"line\">输出</span><br><span class=\"line\"><span class=\"number\">123</span></span><br><span class=\"line\"><span class=\"number\">123</span></span><br><span class=\"line\">Could <span class=\"keyword\">not</span> override field <span class=\"string\">'integer_field'</span> (reference). tom <span class=\"keyword\">is</span> of <span class=\"built_in\">type</span> &lt;<span class=\"keyword\">class</span> <span class=\"string\">'str'</span>&gt; but should be of <span class=\"built_in\">type</span> &lt;<span class=\"keyword\">class</span> <span class=\"string\">'int'</span>&gt; <span class=\"comment\">#报错的地方</span></span><br><span class=\"line\"><span class=\"comment\">#以下输出是所有的cfg的结果，以及嵌套的nested</span></span><br><span class=\"line\">another_integer_field: <span class=\"number\">234</span></span><br><span class=\"line\">float_field: <span class=\"number\">12.0</span></span><br><span class=\"line\">integer_field: <span class=\"number\">123</span></span><br><span class=\"line\">nested:</span><br><span class=\"line\">  string_field: bob</span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n<p>不可以改变的<code>ConfigDict</code></p>\n<figure class=\"highlight python\"><table><tbody><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> ml_collections</span><br><span class=\"line\"><span class=\"comment\">#初始化一个字典</span></span><br><span class=\"line\">initial_dictionary = {</span><br><span class=\"line\">    <span class=\"string\">'int'</span>: <span class=\"number\">1</span>,</span><br><span class=\"line\">    <span class=\"string\">'list'</span>: [<span class=\"number\">1</span>, <span class=\"number\">2</span>],</span><br><span class=\"line\">    <span class=\"string\">'tuple'</span>: (<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>),</span><br><span class=\"line\">    <span class=\"string\">'set'</span>: {<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>},</span><br><span class=\"line\">    <span class=\"string\">'dict_tuple_list'</span>: {<span class=\"string\">'tuple_list'</span>: ([<span class=\"number\">1</span>, <span class=\"number\">2</span>], <span class=\"number\">3</span>)}</span><br><span class=\"line\">}</span><br><span class=\"line\"></span><br><span class=\"line\">cfg = ml_collections.ConfigDict(initial_dictionary)<span class=\"comment\">#把这个字典通过ConfigDict赋值给cfg</span></span><br><span class=\"line\">frozen_dict = ml_collections.FrozenConfigDict(initial_dictionary)<span class=\"comment\">#把这个字典通过FrozenConfigDict赋值给frozen_dict</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(frozen_dict.<span class=\"built_in\">tuple</span>)  <span class=\"comment\"># 输出(1, 2, 3)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(frozen_dict.<span class=\"built_in\">list</span>)  <span class=\"comment\"># 输出 (1, 2)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(frozen_dict.<span class=\"built_in\">set</span>)  <span class=\"comment\"># 输出 {1, 2, 3, 4}</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(frozen_dict.dict_tuple_list.tuple_list[<span class=\"number\">0</span>])  <span class=\"comment\"># 输出 (1, 2)</span></span><br><span class=\"line\"></span><br><span class=\"line\">frozen_cfg = ml_collections.FrozenConfigDict(cfg)<span class=\"comment\">#将cfg变成Forzen类型，即不可再改变其中常量的值</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(frozen_cfg == frozen_dict)  <span class=\"comment\"># True</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">hash</span>(frozen_cfg) == <span class=\"built_in\">hash</span>(frozen_dict))  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">  frozen_dict.<span class=\"built_in\">int</span> = <span class=\"number\">2</span> <span class=\"comment\"># 会报错，因为FrozenConfigDict是不可以改变其中的值的</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> AttributeError <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">  <span class=\"built_in\">print</span>(e)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 在`FrozenConfigDict` 与 `ConfigDict`之间进行转换:</span></span><br><span class=\"line\">thawed_frozen_cfg = ml_collections.ConfigDict(frozen_dict) <span class=\"comment\">#将frozen_dict转化为ConfigDict</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(thawed_frozen_cfg == cfg)  <span class=\"comment\"># True</span></span><br><span class=\"line\">frozen_cfg_to_cfg = frozen_dict.as_configdict()<span class=\"comment\">#将frozen_dict通过as_configdict方法转化为ConfigDict</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(frozen_cfg_to_cfg == cfg)  <span class=\"comment\"># True</span></span><br><span class=\"line\"></span><br></pre></td></tr></tbody></table></figure>\n<h3 id=\"可解释性\">可解释性</h3>\n<p>https://github.com/pytorch/captum</p>\n<p>沙普利值，归因</p>\n","categories":["其他"]},{"title":"迁移目标攻击文献阅读","url":"/posts/bc76/","content":"<h2 id=\"towards-transferable-targeted-attack2020cvpr\">Towards transferable targeted attack（2020CVPR）</h2>\n<p>代码链接：https://github.com/TiJoy/Towards-Transferable-Targeted-Attack</p>\n<span id=\"more\"></span>\n<h3 id=\"摘要\">摘要</h3>\n<p>目标攻击比非目标攻击更难迁移，在本文中分析了两点造成这种困难的原因，1）在迭代攻击中存在梯度消失的现象，在累计动量时，相邻两次成功的噪声过分一致，造成噪声固化；2）目标对抗样本只与目标类别接近还不够，还要远离原始的类别。为解决上述问题引入庞加莱距离（Poincaré distance），作为相似性度量，使得在迭代攻击中梯度大小自适应以缓解噪声固化。进一步，在用度量学习（metric learning）对目标攻击过程正则化处理，使对抗样本远离真实标签并且获得高迁移目标对抗样本。lmageNet上的实验证实了我们的方法的优越性，在黑盒针对性攻击中，与其他最新技术相比,其攻击成功率平均提高了8%</p>\n<h3 id=\"庞加莱距离度量poincare-distance-metric\">庞加莱距离度量（Poincar´e Distance Metric）</h3>\n<p>大多数黑盒攻击使用softmax作为损失函数， <span class=\"math display\">\\[\n\\frac{\\partial L}{\\partial o_{i}}=p_{i}-y_{i}\n\\]</span> <span class=\"math inline\">\\(p_i\\)</span>是预测概率，<span class=\"math inline\">\\(y_i\\)</span>是标签one-hot编码，对于目标攻击，损失函数对softmax输出的向量<strong><span class=\"math inline\">\\(o\\)</span></strong>，在迭代攻击中<span class=\"math inline\">\\(p_i\\)</span>逐渐靠近<span class=\"math inline\">\\(y_i\\)</span>，梯度消失，造成噪声固化，用Poincaré distance替代交叉熵损失可以解决这个问题。</p>\n<p>关于Poincaré distance：</p>\n<p>Poincaré ball上所有点都在一个单位n维<span class=\"math inline\">\\(l_2\\)</span>球上，两点间的距离定义如下： <span class=\"math display\">\\[\nd(u, v)=\\operatorname{arccosh}(1+\\delta(u, v))\n\\]</span> 其中<span class=\"math inline\">\\(u\\)</span>和<span class=\"math inline\">\\(v\\)</span>是n维欧几里得空间<span class=\"math inline\">\\(l_2\\)</span>范数小于1的两点，<span class=\"math inline\">\\(\\delta(u,v)\\)</span>是等距不变量定义如下 <span class=\"math display\">\\[\n\\delta(u, v)=2 \\frac{\\|u-v\\|^{2}}{\\left(1-\\|u\\|^{2}\\right)\\left(1-\\|v\\|^{2}\\right)}\n\\]</span></p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/slbjGoC3k8q1Yx9.png\" alt=\"image-20220329224955169\"><figcaption aria-hidden=\"true\">image-20220329224955169</figcaption>\n</figure>\n<p>图b中任一点到边缘的距离都是<span class=\"math inline\">\\(\\infty\\)</span>，图c中，当接近球的表面时，Poincaré distance增长非常快。这意味着当向表面移动时，梯度大小将会增长。</p>\n<p>由以上定义，对于one-hot编码的标签<span class=\"math inline\">\\(y\\)</span>，有<span class=\"math inline\">\\(\\sum_{i}y_i=1\\)</span>，这表示标签在一个单位<span class=\"math inline\">\\(l_1\\)</span>球上。当<span class=\"math inline\">\\(y\\)</span>是一个未经平滑处理的one-hot标签，有<span class=\"math inline\">\\(\\|y\\|_2=1\\)</span>，所以点<span class=\"math inline\">\\(y\\)</span>在Poincaré ball的边缘，这意味着任一点到<span class=\"math inline\">\\(y\\)</span>的距离都是<span class=\"math inline\">\\(+\\infty\\)</span> 。在目标攻击中，我们的目标是减少模型输出的logits和目标标签之间的距离。</p>\n<p>但使用Poincaré distance存在另外一个问题，融合的logits不满足<span class=\"math inline\">\\(\\|l(x)\\|_2&lt;1\\)</span>，所以本文中logits用<span class=\"math inline\">\\(l_1\\)</span>距离归一化。对于目标标签，任一点到它的距离是+<span class=\"math inline\">\\(\\infty\\)</span> ，这难以优化。将y减去一个非常小的常数<span class=\"math inline\">\\(\\xi=0.0001\\)</span> ，Poincaré distance 度量损失 <span class=\"math display\">\\[\n\\mathcal{L}_{P o}(x, y)=d(u, v)=\\operatorname{arccosh}(1+\\delta(u, v))\n\\]</span> 其中<span class=\"math inline\">\\(u=l_k(x)/\\|l_k(x)\\|_1\\)</span>，<span class=\"math inline\">\\(v=\\max \\{y-\\xi, 0\\}\\)</span>，<span class=\"math inline\">\\(l(x)\\)</span>是融合的logits。在本文中，<span class=\"math inline\">\\(l(x)\\)</span>是融合了多个模型的logits。 <span class=\"math display\">\\[\nl(x)=\\sum_{k=1}^{K} w_{k} l_{k}(x)\n\\]</span> <span class=\"math inline\">\\(K\\)</span>是集成模型数量，<span class=\"math inline\">\\(l_k(x)\\)</span>表示第<span class=\"math inline\">\\(k\\)</span>个模型输出的logits，<span class=\"math inline\">\\(w_k\\)</span>是每个模型的权重，<span class=\"math inline\">\\(w_k&gt;0\\)</span>(<span class=\"math inline\">\\(\\sum_{k=1}^{K}w_k=1\\)</span>) ，在本文中每个模型的权重相等。通过引入Poincaré度量，当且仅当数据接近目标标签时梯度才会变大。</p>\n<h3 id=\"目标攻击三元损失triplet-loss-for-targeted-attack\">目标攻击三元损失（Triplet Loss for Targeted Attack）</h3>\n<p>以往的目标攻击生成的对抗样本和原始类别过于接近，这不利于样本的迁移，本文中引入三元损失，三元损失不仅减少对抗样本和目标类别的距离，还增加对抗样本和原始标签的距离，本文使用干净样本的logits <span class=\"math inline\">\\(l(x_{clean})\\)</span>，one-hot编码的目标标签<span class=\"math inline\">\\(y_{tar}\\)</span>和真实标签<span class=\"math inline\">\\(y_{true}\\)</span>构成三元损失： <span class=\"math display\">\\[\n\\begin{aligned} &amp; \\mathcal{L}_{\\text {trip }}\\left(y_{\\text {tar }}, l\\left(x_{i}\\right), y_{\\text {true }}\\right) \\\\=&amp; {\\left[D\\left(l\\left(x_{i}\\right), y_{\\text {tar }}\\right)-D\\left(l\\left(x_{i}\\right), y_{\\text {true }}\\right)+\\gamma\\right]_{+} . }\\end{aligned}\n\\]</span> 注意到<span class=\"math inline\">\\(l(x_{adv})\\)</span>没有归一化，本文中使用角距离（angular distance）作为距离度量： <span class=\"math display\">\\[\nD\\left(l\\left(x^{a d v}\\right), y_{\\text {tar }}\\right)=1-\\frac{\\left|l\\left(x^{a d v}\\right) \\cdot y_{\\text {tar }}\\right|}{\\left\\|l\\left(x^{a d v}\\right)\\right\\|_{2}\\left\\|y_{\\text {tar }}\\right\\|_{2}} .\n\\]</span> 将三元损失加到损失函数，得到整个的损失函数： <span class=\"math display\">\\[\n\\mathcal{L}_{\\text {all }}=\\mathcal{L}_{P o}\\left(l(x), y_{\\text {tar }}\\right)+\\lambda \\cdot \\mathcal{L}_{\\text {trip }}\\left(y_{\\text {tar }}, l\\left(x_{i}\\right), y_{\\text {true }}\\right)\n\\]</span> 基于MI-FGSM，本文也使用输入多样性的方法，整体算法流程如下图：</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/RbkWgGL7J96VOid.png\" alt=\"image-20220329225043631\"><figcaption aria-hidden=\"true\">image-20220329225043631</figcaption>\n</figure>\n<h3 id=\"实验\">实验</h3>\n<p>实验细节自己看</p>\n<p>参考</p>\n<p>https://blog.csdn.net/sydukee/article/details/110822003</p>\n<p>https://www.jianshu.com/p/afaf551d145e</p>\n<h2 id=\"on-generating-transferable-targeted-perturbationsiccv2021\">On Generating Transferable Targeted Perturbations（ICCV2021）</h2>\n<p>代码链接：https://github.com/Muzammal-Naseer/TTP</p>\n<p>本文提出一种基于生成方法的高迁移目标扰动（transferable targeted perturbations，TTP)，现有的方法依赖于从一个模型到另一个模型的类别边界信息，不利于扰动的迁移。本文的方法匹配扰动图像的\"分布\"和目标类别，最后提出一种新的目标函数不仅对齐原始图像和目标图像的全局分布，也匹配两种域的局部邻域结构。基于所提出的目标，我们训练一个生成器，可以自适应地合成特定于给定输入的扰动。我们的生成方法与源或目标域标签无关，同时对广泛的攻击设定，始终比最先进的方法表现更好。</p>\n<p>主要方法概括：提出一种新的生成训练框架，通过最大化源分布和目标分布在一个预训练的判别器隐空间共同一致性，将源分布映射为目标分布。</p>\n<p>主要贡献</p>\n<ul>\n<li>生成目标迁移：我们提出了一种新的生成方法来学习可迁移的目标对抗扰动。我们独特的训练机制允许生成器在训练期间探索增强的对抗性空间，这提高了推理期间对抗性样本的可转移性。</li>\n<li>互相分布匹配：我们的训练方法是基于给定的源分布和目标分布之间的相互一致。因此，我们的方法可以对训练不需要分类边界信息的生成器提供有针对性的指导。这允许攻击者从无监督的特征中学习有针对性的生成扰动，并消除标记数据的成本。</li>\n<li>邻域相似性匹配：除了全局分布匹配方面，我们在对抗和目标类样本之间引入批处理邻域相似性匹配目标，以最大化两个分布之间的局部对齐。</li>\n</ul>\n<h3 id=\"相关工作\">相关工作</h3>\n<p>基于实例的迭代扰动（PGD,MI,DIM,Po-TRIP,FDA-fd,FDA-N，SGM,LinBP），对每一个样本迭代优化，优化费时并且对于每一个样本独立进行，基于生成的方法虽然需要训练但是可以适应样本，仅通过一次前向传播就可以得到对抗样本。</p>\n<p>目标全局扰动更具有迁移性，基于迭代的方法单次迭代一张图像，缺少特定类别全局信息。需要目标类别标签，只能应用于有监督情况。</p>\n<p><strong>主要迭代攻击（Iterative Attacks）</strong></p>\n<ul>\n<li><p><a href=\"https://arxiv.org/abs/1706.06083\">PGD</a>攻击因过拟合而导致的低迁移性（ICRL-2018）。</p></li>\n<li><p><a href=\"https://arxiv.org/abs/1710.06081\">MI</a>引入动量。它在迭代中累积梯度以减少过拟合（CVPR-2018）。</p></li>\n<li><p><a href=\"https://arxiv.org/abs/1803.06978\">DIM</a>引入输入变换，如填充（padding）或尺度调节（rescaling）以使模式多样化。将其视为减少过拟合的输入空间中的正则项（CVPR-2019）。</p></li>\n<li><p><a href=\"https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Towards_Transferable_Targeted_Attack_CVPR_2020_paper.pdf\">Po-TRIP</a>引入了三元损失，以推动对靶标签的对手示例，同时增加与原始标签的距离（CVPR 2020）。</p></li>\n<li><p><a href=\"https://arxiv.org/abs/2004.12519\">FDA-fd</a>提出了一种在网络的不同层内模拟特征空间中的类别分布的方法。然后从单个最佳层（ICLR 2020）迁移目标扰动。</p></li>\n<li><p><a href=\"https://arxiv.org/abs/2004.14861\">FDA-N</a>横跨多层和分类器（NeurlPS2020）相适用于FDA-FD。</p></li>\n<li><p><a href=\"https://arxiv.org/abs/2002.05990\">SGM</a>发现，在反向传播时，从跳跃连接提供更大的梯度权重增加了可迁移性（ICLR 2020）。</p></li>\n<li><p><a href=\"https://arxiv.org/abs/2012.03528\">LinBP</a>发现线性反向传播可以提高可迁移性（Neurips 2020）。</p></li>\n</ul>\n<p><strong>主要生成攻击（Generative Attacks）</strong></p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/1712.02328\">GAP</a>提出了通过交叉熵训练生成模型抵抗预训练模型的机制（CVPR 2018）。</li>\n<li><a href=\"https://arxiv.org/abs/1905.11736\">CDA</a>提出了通过相对交叉熵训练生成模型抵抗预训练模型的机制（Neurips 2019）。</li>\n<li><a href=\"https://github.com/Muzammal-Naseer/TTP#Citation\">TTP</a>提出基于全局分布匹配目标函数，在一个预训练模型的潜在空间匹配源域和目标域。它不依赖于数据注释（标签）或分类边界信息（ICCV 2021）。</li>\n</ul>\n<h3 id=\"基于生成的目标攻击\">基于生成的目标攻击</h3>\n<h4 id=\"生成模型\">生成模型</h4>\n<p>目标类别<span class=\"math inline\">\\(t\\)</span>，<span class=\"math inline\">\\(P\\)</span>和<span class=\"math inline\">\\(Q\\)</span>分别表示源域和目标域对抗样本，<span class=\"math inline\">\\(x_s\\)</span> ~ <span class=\"math inline\">\\(P\\)</span>，<span class=\"math inline\">\\(x_t\\)</span> ~ <span class=\"math inline\">\\(Q\\)</span>，扰动的源域数据<span class=\"math inline\">\\(P^{\\prime}\\)</span>，<span class=\"math inline\">\\(x^{\\prime}\\)</span> ~ <span class=\"math inline\">\\(P^{'}\\)</span>，<span class=\"math inline\">\\(x^{'}=x_s+\\delta\\)</span>，<span class=\"math inline\">\\(\\mathcal{D}_\\psi(x_s)\\)</span>，<span class=\"math inline\">\\(\\mathcal{D}_\\psi(x^{\\prime}_s)\\)</span>，<span class=\"math inline\">\\(\\mathcal{D}_\\psi(x_t)\\)</span>分别代表潜在分布。</p>\n<p>实验框架如下图，<span class=\"math inline\">\\(\\mathcal{G}_\\theta\\)</span>是生成器，<span class=\"math inline\">\\(\\mathcal{D}_\\theta\\)</span>是判别器，<span class=\"math inline\">\\(\\mathcal{G}_{\\theta}\\)</span>学习源域到目标域的映射，使源域图像获得最小改动，对抗扰动<span class=\"math inline\">\\(\\delta\\)</span>限制在范数距离<span class=\"math inline\">\\(l_{\\infty} \\leq \\epsilon\\)</span>，这通过一个可微的裁剪操作将无界的生成器<span class=\"math inline\">\\(\\mathcal{G}_\\theta\\)</span>生成的对抗扰动投影到距<span class=\"math inline\">\\(x_s\\)</span>固定范数距离内实现。 <span class=\"math display\">\\[\nx^{'}_s=clip(min(x_s+\\epsilon,max(\\mathcal{W}*\\mathcal{G}_\\theta,x_s-\\epsilon))),\\tag 1\n\\]</span> <span class=\"math inline\">\\(\\mathcal{W}\\)</span>是固定权重的平滑操作，在不破坏<span class=\"math inline\">\\(l_{\\infty}\\)</span>范数限制下减少高频信息，使生成器收敛到一个有意义的结果。</p>\n<p><img src=\"https://s2.loli.net/2022/03/30/R8O6i5e3MCQFqvo.png\" alt=\"image-20220329225202825\" style=\"zoom:80%;\"></p>\n<p>原有的基于生成的方法依赖于判别器的分类边界，攻击者必须可以获得一个大量标签数据训练的判别器，进一步生成器依赖于输入的特定实例特征，缺少对全局目标分布特征的把握，造成低迁移性。</p>\n<p>为解决上述问题，本文对目标分布<span class=\"math inline\">\\(Q\\)</span>建模并使在隐层空间<span class=\"math inline\">\\(\\mathcal{D}_{\\psi}\\)</span>中扰动的源域分布<span class=\"math inline\">\\(P^{'}\\)</span>更接近于<span class=\"math inline\">\\(Q\\)</span> <span class=\"math display\">\\[\n\\left\\| \\delta \\right\\|_{\\infty} \\le \\epsilon,     s.t., \\quad \\mathcal{D}_\\psi(x^{'}_s)\\approx \\mathcal{D}_\\psi(x_t)  \\tag 2\n\\]</span> 这个全局的目标函数有两个优势，1）减少扰动和目标分布的不匹配程度，为生成器提供一种引导，2）不需要目标类别信息，判别器可以是一种无监督的方式训练得到</p>\n<h4 id=\"分布匹配\">分布匹配</h4>\n<p>用KL散度表示<span class=\"math inline\">\\(p^{'}\\)</span>和<span class=\"math inline\">\\(Q\\)</span>之间的共同一致性，判别特征<span class=\"math inline\">\\(\\mathcal{D}_\\psi(x^{'}_s)\\)</span>和<span class=\"math inline\">\\(\\mathcal{D}_\\psi(x_t)\\)</span>的KL散度定义如下 <span class=\"math display\">\\[\nD_{K L}\\left(P^{\\prime} \\| Q\\right)=\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{n} \\sigma\\left(\\mathcal{D}_{\\psi}\\left(\\boldsymbol{x}_{s}^{\\prime, i}\\right)\\right)_{j} \\log \\frac{\\sigma\\left(\\mathcal{D}_{\\psi}\\left(\\boldsymbol{x}_{s}^{\\prime, i}\\right)\\right)_{j}}{\\sigma\\left(\\mathcal{D}_{\\psi}\\left(\\boldsymbol{x}_{t}^{i}\\right)\\right)_{j}}\n\\]</span> N代表样本数量，n是判别器输出的特征维数，<span class=\"math inline\">\\(\\sigma\\)</span>表示sigmoid操作，由于KL散度非对称<span class=\"math inline\">\\(D_{K L}\\left( P^{\\prime} \\| Q\\right)\\neq D_{K L}\\left(Q \\| P^{\\prime}\\right)\\)</span>，不是一种距离的度量，损失函数定义 <span class=\"math display\">\\[\n\\mathcal{L}=D_{K L}\\left(P^{\\prime} \\| Q\\right)+D_{K L}\\left(Q \\| P^{\\prime}\\right)\\tag{3}\n\\]</span> 作为一种常规性度量，引入在分布对齐时添加源域样本的增广版本，这能使生成器更关注于特定目标类类别信息，对输入变换更鲁棒。本文中随机使用旋转，裁剪，水平翻转，颜色抖动或者灰度处理变换，从源域<span class=\"math inline\">\\(x_s\\)</span>得到增广样本<span class=\"math inline\">\\(\\widetilde{x}_s\\)</span> 。<span class=\"math inline\">\\(\\widetilde{x}_s\\)</span> ~ <span class=\"math inline\">\\(\\widetilde{P}\\)</span>送入生成器<span class=\"math inline\">\\(\\mathcal{G}_{\\theta}\\)</span> ，扰动的增广样本<span class=\"math inline\">\\(\\widetilde{x}_s ^{'}\\)</span> ~ <span class=\"math inline\">\\(\\widetilde{P}^{'}\\)</span> 用公式（1）投影到增广样本附近，目标域样本没有任何增广操作，<span class=\"math inline\">\\(\\widetilde{x}_s ^{'}\\)</span> 送入判别器计算$ _(^{}<em>s)<span class=\"math inline\">\\(和\\)</span> </em>(x_t)$之间的共同一致性，公式如下 <span class=\"math display\">\\[\n\\mathcal{L}^{a u g}=D_{K L}\\left(\\tilde{P}^{\\prime} \\| Q\\right)+D_{K L}\\left(Q \\| \\tilde{P}^{\\prime}\\right) \\tag 4\n\\]</span></p>\n<h4 id=\"邻域相似度匹配\">邻域相似度匹配</h4>\n<p>上述损失函数没有考虑局部结构，例如一个样本和它增广版本之间的关系，为了扰动的源域和目标类别样本的可靠对齐，提出匹配两种域之间的邻域相似度分布。考虑一批目标域样本<span class=\"math inline\">\\(\\left\\{x_{t}^{i}\\right\\}_{i=1}^{N}\\)</span>和一批扰动的源域样本<span class=\"math inline\">\\(\\left\\{x_{s}^{'i}\\right\\}_{i=1}^{N}\\)</span> ，对于一个训练批次<span class=\"math inline\">\\(x_s ^{'}\\)</span>，计算相似度矩阵<span class=\"math inline\">\\(\\mathcal{S}^s\\)</span>，它的元素编码了原始样本和它的增广版本<span class=\"math inline\">\\(\\widetilde{x}_s ^ {'}\\)</span>之间的余弦相似度。 <span class=\"math display\">\\[\n\\mathcal{S}_{i, j}^{s}=\\frac{\\mathcal{D}_{\\psi}\\left(\\boldsymbol{x}_{s}^{\\prime, i}\\right) \\cdot \\mathcal{D}_{\\psi}\\left(\\tilde{x}_{s}^{\\prime, j}\\right)}{\\left\\|\\mathcal{D}_{\\psi}\\left(x_{s}^{\\prime, i}\\right)\\right\\|\\left\\|\\mathcal{D}_{\\psi}\\left(\\tilde{x}_{s}^{\\prime, j}\\right)\\right\\|} \\tag 5\n\\]</span> 作为对比，对于目标类别样本<span class=\"math inline\">\\(x_t\\)</span> ，计算样本之间的余弦相似度矩阵<span class=\"math inline\">\\(\\mathcal{S}^t\\)</span> <span class=\"math display\">\\[\n\\mathcal{S}_{i, j}^{t}=\\frac{\\mathcal{D}_{\\psi}\\left(x_{t}^{i}\\right) \\cdot \\mathcal{D}_{\\psi}\\left(x_{t}^{j}\\right)}{\\left\\|\\mathcal{D}_{\\psi}\\left(x_{t}^{i}\\right)\\right\\|\\left\\|\\mathcal{D}_{\\psi}\\left(x_{t}^{j}\\right)\\right\\|} \\tag6\n\\]</span> 对上面两个相似度矩阵沿着每一行做softmax操作得到概率估计 <span class=\"math display\">\\[\n\\overline{\\mathcal{S}}_{i, j}=\\frac{\\exp \\left(\\mathcal{S}_{i, j}\\right)}{\\sum_{k} \\exp \\left(\\mathcal{S}_{i, k}\\right)}, where, \\mathcal{S} \\in\\left\\{\\mathcal{S}^{s}, \\mathcal{S}^{t}\\right\\} \\tag 7\n\\]</span> 为了匹配源域和目标域局部邻域模式，计算<span class=\"math inline\">\\(\\overline{\\mathcal{S}}^s\\)</span>和<span class=\"math inline\">\\(\\overline{\\mathcal{S}}^t\\)</span>KL散度作为损失项 <span class=\"math display\">\\[\n\\mathcal{L}^{s i m}=\\sum_{i, j} \\overline{\\mathcal{S}}_{i, j}^{t} \\log \\frac{\\overline{\\mathcal{S}}_{i, j}^{t}}{\\overline{\\mathcal{S}}_{i, j}^{s}}+\\sum_{i, j} \\overline{\\mathcal{S}}_{i, j}^{s} \\log \\frac{\\overline{\\mathcal{S}}_{i, j}^{s}}{\\overline{\\mathcal{S}}_{i, j}^{t}} \\tag 8\n\\]</span></p>\n<h4 id=\"整体损失函数\">整体损失函数</h4>\n<p>生成器参数通过最小化下面损失更新 <span class=\"math display\">\\[\n\\mathcal{L}_{\\mathcal{G}}=\\mathcal{L}+\\mathcal{L}^{a u g}+\\mathcal{L}^{sim } \\tag 9\n\\]</span> 上述损失激励生成器在扰动源域样本时不仅匹配目标分布的全局特征（<span class=\"math inline\">\\(\\mathcal{L}+\\mathcal{L}^{a u g}\\)</span>），而且基于邻域连通匹配局部信息（<span class=\"math inline\">\\(\\mathcal{L}^{sim }\\)</span>）</p>\n<p>整体算法流程</p>\n<p><img src=\"https://s2.loli.net/2022/03/30/L7Y1CTuSEONIcd3.png\" alt=\"image-20220329225416383\" style=\"zoom:80%;\"></p>\n<h3 id=\"实验-1\">实验</h3>\n<p>实验细节自己看吧，对比了很多基于迭代和基于生成的模型</p>\n<h2 id=\"on-success-and-simplicity-a-second-look-at-transferable-targeted-attacksneurips-2021\">On Success and Simplicity: A Second Look at Transferable Targeted Attacks（NeurIPS 2021）</h2>\n<p>代码链接：https://github.com/ZhengyuZhao/Targeted-Tansfer</p>\n<p>本文提出一个简单的logit损失，相比于资源密集型的模型取得了更好的目标迁移效果，在三个新的现实场景：</p>\n<ul>\n<li>低模型相似度的集成迁移（ensemble transfer）</li>\n<li>a worse-case scenario with low-ranked target classes</li>\n<li>Google Cloud Vision API 真实世界的攻击</li>\n</ul>\n<p>中验证了所提方法的优越性，并展示了这个损失对data-free方法生成的目标通用扰动的实用性。</p>\n<h3 id=\"简介\">简介</h3>\n<p>目前目标攻击 resource-intensive transferable attacks可以取得SOTA效果，主要有两种方法：FDA和TTP。FDA的方法[<a href=\"#%5B1%5D\">1</a>][<a href=\"#%5B2%5D\">2</a>]通过在大规模标签数据上训练target-class-specific辅助分类器，然后再深度特征空间中，用辅助分类器优化对抗扰动。TTP[<a href=\"#%5B3%5D\">3</a>]方法基于匹配全局和局部分布训练target-class-specific 对抗生成网络（GANs）,然后使用训练好的生成器对任意输入图像直接生成扰动。</p>\n<p>本文主要贡献在于发现不需要模型训练和外加数据的简单迁移攻击(MI-FGSM，DI-FGSM，TI-FGSM)也能实现强目标迁移性。我们认为这一观点被忽略了，主要是因为目前的研究只使用了少量的迭代次数不合理地限制了攻击收敛性，另一个主要贡献是阐述非常简单的logit loss的普遍优越性，甚至比 resource-intensive SOTA方法表现更好。</p>\n<h3 id=\"对简单迁移攻击的新见解\">对简单迁移攻击的新见解</h3>\n<p>集成三种迁移攻击（MI, DI, TI）可以获得最好效果，其实只使用DI就已经能获得很好的迁移效果，MI和TI和原本效果不好的目标攻击效果差不多。DI比TI表现更好的原因可能是由于DI在每次迭代时随机图像增广而不是TI的固定增广。在这种方式下，对目标攻击的梯度更具有一般性，所以能避免对白盒源模型的过拟合。（？？？不理解）MI和DI，TI有本质区别，它只能固定更新方向，而不能用于向一个特定（目标）实现更准确的梯度方向。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/zwiKEJkBLgqRaCt.png\" alt=\"image-20220329225608693\"><figcaption aria-hidden=\"true\">image-20220329225608693</figcaption>\n</figure>\n<p>​ 图1.</p>\n<p>通常目标迁移攻击限制攻击将攻击优化限制在少数迭代（通常小于20次）。在图1中目标攻击比非目标攻击达到收敛所需要的迭代次数更多。这意味着在只有少数迭代的情况下评估目标迁移性是有问题的。</p>\n<p>现有简单攻击通常使用交叉熵（CE）损失。但随着迭代次数增加，CE损失会造成梯度消失。为解决这个问题，Po+Trip 损失[<a href=\"#%5B4%5D\">4</a>]以一种贪婪的方式随意的逆转梯度的减少，例如在迭代时增大梯度。然而我们发现这种操作导致迭代次数太大，因此造成攻击优化错过最小值。</p>\n<p>在这里，对于损失函数，我们消除了CE损失中使用的最终softmax函数，只是从logit输出中反向传播梯度。 <span class=\"math display\">\\[\nL_{Logit }=-l_{t}\\left(x^{\\prime}\\right) \\tag 1\n\\]</span> <span class=\"math inline\">\\(l_t(·)\\)</span>表示对目标类别 logit的输出。虽然用logit攻击的想法并不新，但它在目标迁移性方面的优越性能至今还没有被认识。我们还发现，使用著名的基于logit的损失，C&amp;W，得到的结果一直比较糟糕。</p>\n<p>下面公式<span class=\"math inline\">\\(\\eqref{eq22}\\)</span>展示logit损失比CE损失有更强的梯度。CE损失对目标logit输入<span class=\"math inline\">\\(z_t\\)</span>的梯度对<span class=\"math inline\">\\(p_t\\)</span>单调递减，因为使用softmax函数，<span class=\"math inline\">\\(p_t\\)</span>很快到达1，因此梯度几乎消失。这种现象使得即使应用更多的迭代，也很难改善攻击。公式<span class=\"math inline\">\\(\\eqref{eq3}\\)</span>logit损失是一个常数。 <span class=\"math display\">\\[\n\\begin{aligned} L_{C E} &amp;=-1 \\cdot \\log \\left(p_{t}\\right)=-\\log \\left(\\frac{e^{z_{t}}}{\\sum e^{z_{j}}}\\right)=-z_{t}+\\log \\left(\\sum e^{z_{j}}\\right) \\\\ \\frac{\\partial L_{C E}}{\\partial z_{t}} &amp;=-1+\\frac{\\partial \\log \\left(\\sum e^{z_{j}}\\right)}{\\partial e^{z_{t}}} \\cdot \\frac{\\partial e^{z_{t}}}{\\partial z_{t}}=-1+\\frac{e^{z_{t}}}{\\sum e^{z_{j}}}=-1+p_{t} \\end{aligned} \\tag 2\\label {eq22}\n\\]</span></p>\n<p><span class=\"math display\">\\[\nL_{\\text {Logit }}=-z_{t}, \\frac{\\partial L_{\\text {Logit }}}{\\partial z_{t}}=-1 \\tag 3 \\label{eq3}\n\\]</span></p>\n<h3 id=\"实验-2\">实验</h3>\n<p>实验数据集：NIPS 2017对抗攻击和防御<a href=\"https://github.com/cleverhans-lab/cleverhans/tree/master/cleverhans_v3.%201.0/examples/nips17_adversarial_competition/dataset\">数据集</a>，在TI中<span class=\"math inline\">\\(\\|W\\|_1=5\\)</span>，所有攻击迭代次数300。</p>\n<h4 id=\"单个模型迁移效果\">单个模型迁移效果</h4>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/WqwZuzsS2vP43ky.png\" alt=\"image-20220329225646394\"><figcaption aria-hidden=\"true\">image-20220329225646394</figcaption>\n</figure>\n<p>目标模型是resnet和densenet时目标迁移攻击成功率较高，一种解释是网络中的跳跃连接（skip connection）提高迁移率[<a href=\"#%22%5B5%5D%22\">5</a>,<a href=\"#%22%5B6%5D%22\">6</a>]，目标模型是Inception-v3 迁移成功率较低，一个可能的原因是Inception 结构有多尺度卷积和两个辅助分类器。</p>\n<h4 id=\"在简单和有挑战性场景下的集成迁移\">在简单和有挑战性场景下的集成迁移</h4>\n<p>（1）在简单情况下Po+Trip 在迭代次数限制在20次时效果比CE好，在足够迭代次数效果变差，下表对应20/100迭代次数。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/9J67qMVSLcWohnI.png\" alt=\"image-20220329225711185\"><figcaption aria-hidden=\"true\">image-20220329225711185</figcaption>\n</figure>\n<p>（2）在源模型和目标模型没有交叉，logit方法攻击效果比CE和Po+Trip 有显著提升。三种方法比单个模型情况都有本质提升。Po+Trip方法在一些情况下表现最差因为它随机增加梯度大小，因为模型的多样性，损失面不光滑，造成优化过程错过最小值点。和单个模型的迁移场景一样，这种情况下Inception-v3的迁移也是最困难的。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/vCRMBWNakojnhzE.png\" alt=\"image-20220329225950346\"><figcaption aria-hidden=\"true\">image-20220329225950346</figcaption>\n</figure>\n<p>（3）目前基于迁移攻击的评估仅限于攻击最佳和平均的情况，为了解决这一局限性，我们考虑了一种更坏的转移情况，通过将目标从最高排序类别逐渐改变为最低排序的类别。在下表中，目标类别排名位置和目标迁移率有不可忽略的关系。具体地说，当目标标签预测概率越低，攻击越困难，排名较高的目标的结果可能不会像排名较低的目标的更现实、更糟糕的情况那样揭示不同攻击的实际强度。特别是，只看排名最高的目标的最佳情况可能会导致一个误导性的结论，即CE是最有效的攻击。这一发现表明，对目标可迁移性的更有意义的评估应进一步增加难度，超越目前的最佳和平均情况。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/PaUwECf2sWqDSvH.png\" alt=\"image-20220329225826328\"><figcaption aria-hidden=\"true\">image-20220329225826328</figcaption>\n</figure>\n<p>（4）对Google Cloud Vision迁移攻击。API预测了一组语义标签和置信度分数。具体来说，只返回顶部置信度不低于50%的类，最多显示10个类。请注意这里的置信度分数不是概率(总和为1)。我们测试了目标和非目标的可迁移性。由于所有返回的标签都具有较高的可信度(<span class=\"math inline\">\\(\\ge\\)</span> 50%)，我们不会将成功率的衡量限制在top-1类别。相反，对于非目标成功率，我们测试真实标签是否出现在返回的列表中，而对于目标成功率，测试目标类是否出现。由于API预测的语义标签集并不完全对应1000个ImageNet类，所以我们将语义相似的类视为同一个类。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/zRqbiPowtdGYmMF.png\" alt=\"image-20220329225851266\"><figcaption aria-hidden=\"true\">image-20220329225851266</figcaption>\n</figure>\n<p>表4报告了超过100张图像的平均结果，这些图像最初产生的是正确的预测。可以看出，总的来说，实现目标转移的成功要比非目标转移的成功困难得多。特别是，Logit攻击实现了最佳的目标可迁移性，图4中显示了准不可察觉的扰动。我们的研究结果揭示了Google Cloud Vision在简单的基于迁移的攻击中存在的潜在漏洞，这种攻击不需要查询交互。</p>\n<h4 id=\"logit方法和resource-intensive迁移攻击方法对比\">Logit方法和Resource-Intensive迁移攻击方法对比</h4>\n<p>（1）和TTP对比</p>\n<p>参数：10-Targets（all-source），单个模型情况：ResNet50，集成模型情况ResNet{18,50,101,152}。目标模型：DenseNet121 和 VGG16 bn。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/qEYofU9DKepFWha.png\" alt=\"image-20220329230031257\"><figcaption aria-hidden=\"true\">image-20220329230031257</figcaption>\n</figure>\n<p>在上表中，在一般情况下<span class=\"math inline\">\\(\\epsilon=16\\)</span>，logit攻击能获得和TTP差不多的效果，集成模型对logit攻击提升更大，这可能是因为即使使用单模型作为判别器，TTP也可以通过训练很好地学习目标语义的特征和通过大规模数据学习目标类别分布。图5. 通过比较TTP实现的无边界扰动与Logit实现的无界扰动，可以确定TTP学习到的更清晰的目标语义。</p>\n<p>（2）和<span class=\"math inline\">\\(\\text{FDA}^{N}\\)</span>+xent方法对比</p>\n<p>CE, Po+Trip, 和 Logit三种方法和<span class=\"math inline\">\\(\\text{FDA}^{N}\\)</span>+xent 在生成无边界对抗图像对比，对抗扰动初始化为随机高斯噪声并且要求越大越好。在4000张图像上实验，每一张图像朝着随机目标优化。MI迁移模型在无边界情况下有损表现，去除。表6所有的三种简单迁移攻击都比<span class=\"math inline\">\\(\\text{FDA}^{N}\\)</span>+xent的方法要好，在图5中无边界的对抗扰动能够在某种程度上反应目标的语义。这一发现表明，实现目标可迁移性依赖于鲁棒的，语义特征，这些特征被各种模型所学习，也被人类所理解。这样，实现有目标的可迁移性与非有目标的可迁移性有本质区别，非目标攻击攻击非鲁棒特征就足够。同样值得注意的是，在具有较小范数边界的实际场景中，语义对齐扰动不改变人类的判断。</p>\n<figure>\n<img src=\"https://s2.loli.net/2022/03/30/GlYAviuMJERagNz.png\" alt=\"image-20220329230110609\"><figcaption aria-hidden=\"true\">image-20220329230110609</figcaption>\n</figure>\n<p>（3）简单Logit攻击在Data-Free通用扰动应用</p>\n<p>上述对扰动的观察可以反映出特定的目标语义激励我们应用Logit攻击来实现目标的通用对抗扰动(UAPs)，这可以将多个原始图像分类到特定的目标类。现有的实现目标通用扰动主要依赖于对附加数据的大规模优化。然而，简单logit攻击能容易的扩展到data-free目标通用扰动，上述迁移logit攻击和唯一区别是用平均图像（所有像素值设为0.5而不是[0, 1]）作为原始图像。</p>\n<p>在我们的实验中，对于每个目标类，我们进行300次迭代生成一个单一的目标UAP向量(<span class=\"math inline\">\\(\\epsilon\\)</span> = 16)，并将其应用到我们数据集中的所有1000张图像。表7报告了所有1000个ImageNet类的平均结果。可以看出，logit loss可以带来实质性的成功，明显优于CE损失。图6可以确认这一点，图6显示了logit攻击可以产生比CE更多的语义对齐的扰动。这个观察结果也支持来自[<a href=\"#%22%5B7%5D%22\">7</a>]的结论，即通用扰动包含域特征，而图像就扰动而言就像噪音。</p>\n<h3 id=\"结论和回顾\">结论和回顾</h3>\n<p>在这篇论文中，我们已经证明了实现有针对性的可迁移并不像目前的工作总结的那么困难。具体地说，我们发现，当给定足够的收敛迭代时，简单的可转移攻击实际上可以实现惊人的目标可迁移性。我们已经在广泛的场景中验证了简单可迁移攻击的有效性，包括三个新引入的具有挑战性的场景。这些具有挑战性的场景更好地揭示了不同攻击的实际能力。特别地，我们证明了一个非常简单的Logit攻击在所有迁移场景中都具有优越性，可以获得比最先进的资源密集型（resource-intensive）方法更好的结果。我们还展示了Logit攻击在以无数据方式产生目标通用对抗扰动方面的潜在应用。总的来说，我们希望我们的研究成果能够对未来的研究工作有所启发，对目标可迁移性进行更有意义的评估。我们未来的工作将集中于研究为什么不同的模型架构产生不同的可移植性。特别的我们应该去探索Inception-v3的低成功率。接下来，需要从多个方面对不同攻击的资源消耗进行更全面的讨论，例如训练和推理时间、硬件资源和数据大小。</p>\n<p>较强的可移动性明显有利于对抗性图像在黑盒社交公益上应用。此外，考虑到我们的发现，即使是简单的攻击也可以产生高度可迁移的对抗图像，这也将激励社区设计更强大的防御。我们的方法仍有可能被恶意行为者滥用，以破坏合法的系统。然而，我们坚信，我们的论文能够为研究人员提供的帮助远远大于它可能为一个实际的恶意行动者提供的帮助。</p>\n<p>参考文献</p>\n<p><span id=\"[1]\">[1] Nathan Inkawhich, Kevin Liang, Lawrence Carin, and Yiran Chen. Transferable perturbations of deep feature distributions. In ICLR, 2020.</span></p>\n<p><span id=\"[2]\">[2] Nathan Inkawhich, Kevin J Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin, and Yiran Chen. Perturbing across the feature hierarchy to improve standard and strict blackbox attack transferability. In NeurIPS, 2020.</span></p>\n<p><span id=\"[3]\">[3] Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. On generating transferable targeted perturbations. In ICCV, 2021.</span></p>\n<p><span id=\"[4]\">[4] Maosen Li, Cheng Deng, Tengjiao Li, Junchi Yan, Xinbo Gao, and Heng Huang. Towards transferable targeted attack. In CVPR, 2020.</span></p>\n<p><span id=\"[5]\">[5] Nathan Inkawhich, Kevin Liang, Lawrence Carin, and Yiran Chen. Transferable perturbations of deep feature distributions. In ICLR, 2020.</span></p>\n<p><span id=\"[6]\">[6] Nathan Inkawhich, Kevin J Liang, Binghui Wang, Matthew Inkawhich, Lawrence Carin, and Yiran Chen. Perturbing across the feature hierarchy to improve standard and strict blackbox attack transferability. In NeurIPS, 2020.</span></p>\n<p><span id=\"[7]\">[7] Chaoning Zhang, Philipp Benz, Tooba Imtiaz, and In So Kweon. Understanding adversarial examples from the mutual influence of images and perturbations. In CVPR, 2020.</span></p>\n","categories":["攻击","目标攻击"],"tags":["对抗攻击"]},{"title":"Yet Another Intermediate-Level Attack","url":"/posts/382/","content":"<p>Yet Another Intermediate-Level Attack（ECCV 2020）</p>\n<p>代码：https://github.com/qizhangli/ila-plus-plus</p>\n<p>本篇也是利用网络中间层进行攻击，是[<a href=\"#%5B1%5D\">1</a>]的延续。本文的主要思想是建立一个由中间层特征到对抗损失的映射<span class=\"math inline\">\\(W\\)</span>，生成具有最大预期对抗损失的像素级扰动。在本文中定的对抗扰动是交叉熵损失。作者认为映射<span class=\"math inline\">\\(W\\)</span>包含了baseline攻击方法的每一步的信息，比单独使用最后一次迭代的结果更具有引导作用。</p>\n<span id=\"more\"></span>\n<h2 id=\"主要方法\">主要方法</h2>\n<p>由上面的动机可知本文的关键是如何利用baseline方法每一步的信息构建映射<span class=\"math inline\">\\(W\\)</span>。假设中间层特征用<span class=\"math inline\">\\(h_t^{adv}=g(x_t^{adv})\\)</span>表示。baseline每一步生成的对抗样本<span class=\"math inline\">\\(x_0^{adv},...x_t^{adv},...x_{p}^{adv}\\)</span>，可以得到一系列的中间层差异和对抗损失<span class=\"math inline\">\\(\\{(h_t^{adv}-h_0^{adv},l_t)\\}\\)</span>，建立一个映射<span class=\"math inline\">\\(W\\)</span>直接由中间层差异预测对抗损失，<span class=\"math inline\">\\(W\\)</span>可以看成下面的优化问题： <span class=\"math display\">\\[\n\\min _{\\mathbf{w}} \\sum_{t=0}^p\\left(\\mathbf{w}^T\\left(\\mathbf{h}_t^{adv}-\\mathbf{h}_0^{adv}\\right)-l_t\\right)^2+\\lambda\\|\\mathbf{w}\\|^2 \\tag{1}\n\\]</span> <span class=\"math inline\">\\(W\\in \\mathbb{R}^m，H\\in \\mathbb{R}^{(p+1)\\times m}，r \\in \\mathbb{R}^{p+1}\\)</span>，上面优化问题写成向量/矩阵形式： <span class=\"math display\">\\[\n\\min _{\\mathbf{w}} \\|\\mathbf{r}-\\mathbf{HW}\\|^2+\\lambda\\|\\mathbf{W}\\|^2\n\\]</span> 上述问题有闭式解<span class=\"math inline\">\\(\\mathbf{W^{\\star}}=(\\mathbf{H^{\\top}H+\\lambda I_m})^{-1}\\mathbf{H^{\\top}r}\\)</span>，整个优化过程： <span class=\"math display\">\\[\n\\max _{\\Delta_w}\\left(g\\left(\\mathbf{x}+\\Delta_x\\right)-\\mathbf{h}_0^{\\text {adv }}\\right)^{\\top} \\mathbf{w}^{\\star}, \\quad s.t. \\left(\\mathbf{x}+\\boldsymbol{\\Delta}_x\\right) \\in \\Psi\n\\]</span> <span class=\"math inline\">\\((\\mathbf{H^{\\top}H+\\lambda I_m}) \\in \\mathbb{R}^{m\\times m}\\)</span><span class=\"math inline\">\\(p\\)</span>，计算它的逆矩阵困难，<span class=\"math inline\">\\(m\\)</span>一般都远大于<span class=\"math inline\">\\(p\\)</span>，使用Woodbury等式变换 <span class=\"math display\">\\[\n\\begin{aligned}\n\\mathrm{H}^{\\top} \\mathrm{H}+\\lambda \\mathrm{I}_m &amp; =\\frac{1}{\\lambda} I-\\frac{1}{\\lambda^2} \\mathrm{H}^{\\top}\\left(\\frac{1}{\\lambda} \\mathrm{HH}^{\\top}+\\mathrm{I}_p\\right)^{-1} \\mathrm{H} \\\\\n&amp; =\\frac{1}{\\lambda} I-\\frac{1}{\\lambda} \\mathrm{H}^{\\top}\\left(\\mathrm{HH}^{\\top}+\\lambda \\mathrm{I}_p\\right)^{-1} \\mathrm{H}\n\\end{aligned}\n\\]</span> 现在只需要计算<span class=\"math inline\">\\(\\mathbf{HH^{\\top}+\\lambda I_p}\\)</span>的逆即可，当正则化参数<span class=\"math inline\">\\(\\lambda\\)</span>非常大时，<span class=\"math inline\">\\(\\mathrm{H}^{\\top}\\left(\\mathrm{HH}^{\\top}+\\lambda \\mathrm{I}_p\\right)^{-1} \\mathrm{H}\\approx 0\\)</span>，这种情况下优化问题近似于$ _{_w}(g(+_x)-_0^{a dv} )^{} ^{} <span class=\"math inline\">\\(，如果只考虑\\)</span>x_p<sup>{adv}<span class=\"math inline\">\\(和干净样本的差异或者baseline时单步攻击，优化问题退化为ILA。\\)</span></sup>{} $也可以看作是一系列投影方向的线性组合，充分利用baseline的时序信息提高迁移能力。</p>\n<p>考虑到不同迭代步数是中间层差异数值差异较大，在求解公式1之前对中间层差异进行归一化，用<span class=\"math inline\">\\(\\tilde{H}\\)</span>代替<span class=\"math inline\">\\(H\\)</span>，<span class=\"math inline\">\\(\\tilde{H}\\)</span>第<span class=\"math inline\">\\(t\\)</span>行表示为<span class=\"math inline\">\\((h_t^{adv}-h_t^{0})/\\|h_t^{adv}-h_t^{0}\\|\\)</span>。</p>\n<figure>\n<img src=\"https://s2.loli.net/2023/02/15/kcqU8g2VhBy3Crj.png\" alt=\"image-20230215164956294\"><figcaption aria-hidden=\"true\">image-20230215164956294</figcaption>\n</figure>\n<h2 id=\"总结与改进\">总结与改进</h2>\n<p>充分利用baseline每步迭代信息值得借鉴。</p>\n<p><strong>参考文献</strong></p>\n<p><a id=\"[1]\">[1] Enhancing adversarial example transferability with an intermediate level attack.&nbsp;</a></p>\n","categories":["对抗攻击"],"tags":["对抗攻击"]}]